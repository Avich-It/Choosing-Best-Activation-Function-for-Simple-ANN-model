{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Optimized_and_Tuned_simple_ANN_Model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "3s9bl3ItPJ9x"
      },
      "source": [
        "#Importing required libraries\n",
        "import tensorflow as tf\n",
        "from keras.utils import np_utils\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import datasets\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras import optimizers\n",
        "from keras.callbacks import History\n",
        "\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bIE7fWtUOIt",
        "outputId": "cb790d1f-fe3f-4518-cbe6-f4edd39c22f7"
      },
      "source": [
        "#Test Train Split\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)\n",
        "#plt.imshow(X_train[1170])\n",
        "#plt.show()\n",
        "\n",
        "input_dim=x_train.shape[1]\n",
        "print(input_dim)"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n",
            "(10000, 28, 28)\n",
            "28\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OGb2UtGnfSC"
      },
      "source": [
        "# Build Model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras import optimizers\n",
        "\n",
        "# Function to create model, required for KerasClassifier\n",
        "def create_model(activation='relu',\n",
        "                 optimizer='adam'):\n",
        "    \n",
        "    # Create model\n",
        "    model = Sequential()\n",
        "    # Input\n",
        "    model.add(Flatten())\n",
        "    # Hidden\n",
        "    model.add(Dense(128, activation=activation))\n",
        "    # Output\n",
        "    model.add(Dense(10,activation='softmax'))\n",
        "    \n",
        "    # Compile model\n",
        "    model.compile(loss='categorical_crossentropy', \n",
        "                  optimizer=optimizer, \n",
        "                  metrics=['accuracy'])\n",
        "    return model"
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ihIWMENXtWA_",
        "outputId": "0a1b4509-c4e2-43c2-f08d-c84811729f42"
      },
      "source": [
        "# Create model using Keras Claasifier\n",
        "kmodel = KerasClassifier(build_fn=create_model)\n",
        "kmodel.fit(x_train, y_train, epochs=5)"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 6.9412 - accuracy: 0.8177\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.3929 - accuracy: 0.9076\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.2894 - accuracy: 0.9275\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.2387 - accuracy: 0.9400\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.2278 - accuracy: 0.9397\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fcef570cdd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Hmh9x5CvotH"
      },
      "source": [
        "# Specify Hyperparameters and Model Design Components\n",
        "activation =  ['relu', 'elu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear']\n",
        "optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
        "epochs = [10,20,30,40,50]\n",
        "batch_size = [1024,2048]\n",
        "param_dist = dict (activation=activation,\n",
        "                   optimizer=optimizer,\n",
        "                   epochs=epochs,\n",
        "                   batch_size=batch_size)"
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P79sR1aQwACw",
        "outputId": "b3de1b56-8551-42df-8733-d2d29b8d93a0"
      },
      "source": [
        "# Perform Randomized search\n",
        "n_iter_search = 10\n",
        "random_search = RandomizedSearchCV(estimator=kmodel, \n",
        "                                   param_distributions=param_dist,\n",
        "                                   n_iter=n_iter_search,\n",
        "                                   n_jobs=1, cv=3,\n",
        "                                   scoring='accuracy')\n",
        "random_search.fit(x_test, y_test)"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 2.6746 - accuracy: 0.1661\n",
            "Epoch 2/20\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 1.8168 - accuracy: 0.3613\n",
            "Epoch 3/20\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 1.4290 - accuracy: 0.5384\n",
            "Epoch 4/20\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 1.1865 - accuracy: 0.6410\n",
            "Epoch 5/20\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 1.0048 - accuracy: 0.6974\n",
            "Epoch 6/20\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.9001 - accuracy: 0.7350\n",
            "Epoch 7/20\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.8140 - accuracy: 0.7658\n",
            "Epoch 8/20\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.7499 - accuracy: 0.7901\n",
            "Epoch 9/20\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.7008 - accuracy: 0.8067\n",
            "Epoch 10/20\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6538 - accuracy: 0.8197\n",
            "Epoch 11/20\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6245 - accuracy: 0.8286\n",
            "Epoch 12/20\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5857 - accuracy: 0.8394\n",
            "Epoch 13/20\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5656 - accuracy: 0.8449\n",
            "Epoch 14/20\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5403 - accuracy: 0.8573\n",
            "Epoch 15/20\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5200 - accuracy: 0.8614\n",
            "Epoch 16/20\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5076 - accuracy: 0.8649\n",
            "Epoch 17/20\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.4838 - accuracy: 0.8748\n",
            "Epoch 18/20\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.4722 - accuracy: 0.8786\n",
            "Epoch 19/20\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.4616 - accuracy: 0.8824\n",
            "Epoch 20/20\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.4464 - accuracy: 0.8889\n",
            "Epoch 1/20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 0s 5ms/step - loss: 2.6807 - accuracy: 0.1406\n",
            "Epoch 2/20\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 1.8479 - accuracy: 0.3774\n",
            "Epoch 3/20\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 1.4802 - accuracy: 0.5185\n",
            "Epoch 4/20\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 1.2730 - accuracy: 0.5977\n",
            "Epoch 5/20\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 1.1187 - accuracy: 0.6615\n",
            "Epoch 6/20\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 1.0073 - accuracy: 0.6945\n",
            "Epoch 7/20\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.9305 - accuracy: 0.7229\n",
            "Epoch 8/20\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.8606 - accuracy: 0.7479\n",
            "Epoch 9/20\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.8076 - accuracy: 0.7644\n",
            "Epoch 10/20\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.7614 - accuracy: 0.7832\n",
            "Epoch 11/20\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7136 - accuracy: 0.7987\n",
            "Epoch 12/20\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6768 - accuracy: 0.8099\n",
            "Epoch 13/20\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6477 - accuracy: 0.8189\n",
            "Epoch 14/20\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6230 - accuracy: 0.8302\n",
            "Epoch 15/20\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5970 - accuracy: 0.8379\n",
            "Epoch 16/20\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5730 - accuracy: 0.8442\n",
            "Epoch 17/20\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5634 - accuracy: 0.8474\n",
            "Epoch 18/20\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.5423 - accuracy: 0.8562\n",
            "Epoch 19/20\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5186 - accuracy: 0.8644\n",
            "Epoch 20/20\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5097 - accuracy: 0.8664\n",
            "Epoch 1/20\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 2.8338 - accuracy: 0.1251\n",
            "Epoch 2/20\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 1.8946 - accuracy: 0.3498\n",
            "Epoch 3/20\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 1.4905 - accuracy: 0.5123\n",
            "Epoch 4/20\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 1.2520 - accuracy: 0.6063\n",
            "Epoch 5/20\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 1.1033 - accuracy: 0.6607\n",
            "Epoch 6/20\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.9970 - accuracy: 0.6956\n",
            "Epoch 7/20\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.9143 - accuracy: 0.7238\n",
            "Epoch 8/20\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.8607 - accuracy: 0.7444\n",
            "Epoch 9/20\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.7948 - accuracy: 0.7721\n",
            "Epoch 10/20\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.7520 - accuracy: 0.7840\n",
            "Epoch 11/20\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.7268 - accuracy: 0.7895\n",
            "Epoch 12/20\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6909 - accuracy: 0.8036\n",
            "Epoch 13/20\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6643 - accuracy: 0.8158\n",
            "Epoch 14/20\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6404 - accuracy: 0.8229\n",
            "Epoch 15/20\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6187 - accuracy: 0.8299\n",
            "Epoch 16/20\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.5970 - accuracy: 0.8368\n",
            "Epoch 17/20\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5762 - accuracy: 0.8433\n",
            "Epoch 18/20\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5652 - accuracy: 0.8460\n",
            "Epoch 19/20\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5417 - accuracy: 0.8531\n",
            "Epoch 20/20\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.5403 - accuracy: 0.8540\n",
            "Epoch 1/20\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 136.8841 - accuracy: 0.2032\n",
            "Epoch 2/20\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 24.1435 - accuracy: 0.6104\n",
            "Epoch 3/20\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 16.5360 - accuracy: 0.7102\n",
            "Epoch 4/20\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 9.4205 - accuracy: 0.7927\n",
            "Epoch 5/20\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 6.9441 - accuracy: 0.8355\n",
            "Epoch 6/20\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 5.8295 - accuracy: 0.8495\n",
            "Epoch 7/20\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 5.1320 - accuracy: 0.8624\n",
            "Epoch 8/20\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 6.9789 - accuracy: 0.8303\n",
            "Epoch 9/20\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 5.9402 - accuracy: 0.8497\n",
            "Epoch 10/20\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 2.8953 - accuracy: 0.9140\n",
            "Epoch 11/20\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 2.9892 - accuracy: 0.9050\n",
            "Epoch 12/20\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 4.0295 - accuracy: 0.8883\n",
            "Epoch 13/20\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 3.2725 - accuracy: 0.9097\n",
            "Epoch 14/20\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 1.6861 - accuracy: 0.9354\n",
            "Epoch 15/20\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.4002 - accuracy: 0.9437\n",
            "Epoch 16/20\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 4.4203 - accuracy: 0.8818\n",
            "Epoch 17/20\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 1.0039 - accuracy: 0.9589\n",
            "Epoch 18/20\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.8277 - accuracy: 0.9623\n",
            "Epoch 19/20\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 4.4253 - accuracy: 0.8896\n",
            "Epoch 20/20\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.1887 - accuracy: 0.9481\n",
            "WARNING:tensorflow:5 out of the last 633 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fcef20db680> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Epoch 1/20\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 138.7937 - accuracy: 0.2048\n",
            "Epoch 2/20\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 37.5562 - accuracy: 0.5587\n",
            "Epoch 3/20\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 15.5920 - accuracy: 0.7065\n",
            "Epoch 4/20\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 10.1166 - accuracy: 0.7740\n",
            "Epoch 5/20\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 8.1440 - accuracy: 0.8050\n",
            "Epoch 6/20\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 8.7849 - accuracy: 0.8003\n",
            "Epoch 7/20\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 5.6854 - accuracy: 0.8548\n",
            "Epoch 8/20\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 6.6656 - accuracy: 0.8350\n",
            "Epoch 9/20\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 9.9757 - accuracy: 0.8113\n",
            "Epoch 10/20\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 3.4054 - accuracy: 0.9034\n",
            "Epoch 11/20\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 3.1739 - accuracy: 0.9031\n",
            "Epoch 12/20\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 5.9123 - accuracy: 0.8413\n",
            "Epoch 13/20\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 2.7960 - accuracy: 0.9104\n",
            "Epoch 14/20\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 3.6751 - accuracy: 0.8943\n",
            "Epoch 15/20\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 3.5170 - accuracy: 0.9009\n",
            "Epoch 16/20\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.3355 - accuracy: 0.9522\n",
            "Epoch 17/20\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 1.2669 - accuracy: 0.9499\n",
            "Epoch 18/20\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 4.3189 - accuracy: 0.8746\n",
            "Epoch 19/20\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 1.6141 - accuracy: 0.9417\n",
            "Epoch 20/20\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.8764 - accuracy: 0.9601\n",
            "WARNING:tensorflow:6 out of the last 635 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fcefc704b90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Epoch 1/20\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 156.7917 - accuracy: 0.1719\n",
            "Epoch 2/20\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 41.3113 - accuracy: 0.4979\n",
            "Epoch 3/20\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 16.7097 - accuracy: 0.6564\n",
            "Epoch 4/20\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 12.5366 - accuracy: 0.7097\n",
            "Epoch 5/20\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 9.4898 - accuracy: 0.7635\n",
            "Epoch 6/20\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 10.3567 - accuracy: 0.7556\n",
            "Epoch 7/20\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 9.0898 - accuracy: 0.7893\n",
            "Epoch 8/20\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 4.9233 - accuracy: 0.8530\n",
            "Epoch 9/20\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 4.8734 - accuracy: 0.8506\n",
            "Epoch 10/20\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 6.1414 - accuracy: 0.8275\n",
            "Epoch 11/20\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 4.3474 - accuracy: 0.8618\n",
            "Epoch 12/20\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 3.1562 - accuracy: 0.8897\n",
            "Epoch 13/20\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 3.7731 - accuracy: 0.8778\n",
            "Epoch 14/20\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 5.7454 - accuracy: 0.8410\n",
            "Epoch 15/20\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 1.9893 - accuracy: 0.9211\n",
            "Epoch 16/20\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 1.4335 - accuracy: 0.9328\n",
            "Epoch 17/20\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.4128 - accuracy: 0.9320\n",
            "Epoch 18/20\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 1.2886 - accuracy: 0.9334\n",
            "Epoch 19/20\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 6.2127 - accuracy: 0.8507\n",
            "Epoch 20/20\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 1.2104 - accuracy: 0.9454\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fcef9bdd290> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Epoch 1/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 171.8311 - accuracy: 0.1068\n",
            "Epoch 2/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 170.8895 - accuracy: 0.1067\n",
            "Epoch 3/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 170.2085 - accuracy: 0.1071\n",
            "Epoch 4/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 170.7474 - accuracy: 0.1040\n",
            "Epoch 5/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 167.9708 - accuracy: 0.1084\n",
            "Epoch 6/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 167.9995 - accuracy: 0.1055\n",
            "Epoch 7/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 167.4877 - accuracy: 0.1074\n",
            "Epoch 8/50\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 167.2779 - accuracy: 0.1062\n",
            "Epoch 9/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 166.3379 - accuracy: 0.1080\n",
            "Epoch 10/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 165.2701 - accuracy: 0.1075\n",
            "Epoch 11/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 163.6104 - accuracy: 0.1124\n",
            "Epoch 12/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 163.3407 - accuracy: 0.1086\n",
            "Epoch 13/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 162.4093 - accuracy: 0.1110\n",
            "Epoch 14/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 161.1851 - accuracy: 0.1108\n",
            "Epoch 15/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 161.3540 - accuracy: 0.1107\n",
            "Epoch 16/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 161.6171 - accuracy: 0.1085\n",
            "Epoch 17/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 160.3202 - accuracy: 0.1115\n",
            "Epoch 18/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 160.1818 - accuracy: 0.1104\n",
            "Epoch 19/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 159.4817 - accuracy: 0.1085\n",
            "Epoch 20/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 158.7774 - accuracy: 0.1095\n",
            "Epoch 21/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 158.7154 - accuracy: 0.1077\n",
            "Epoch 22/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 156.4729 - accuracy: 0.1094\n",
            "Epoch 23/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 156.4519 - accuracy: 0.1115\n",
            "Epoch 24/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 156.8356 - accuracy: 0.1089\n",
            "Epoch 25/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 155.5549 - accuracy: 0.1096\n",
            "Epoch 26/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 154.6680 - accuracy: 0.1089\n",
            "Epoch 27/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 153.7720 - accuracy: 0.1129\n",
            "Epoch 28/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 154.2805 - accuracy: 0.1124\n",
            "Epoch 29/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 153.7018 - accuracy: 0.1113\n",
            "Epoch 30/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 153.6663 - accuracy: 0.1083\n",
            "Epoch 31/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 153.2326 - accuracy: 0.1066\n",
            "Epoch 32/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 152.4744 - accuracy: 0.1091\n",
            "Epoch 33/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 150.7499 - accuracy: 0.1111\n",
            "Epoch 34/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 150.1345 - accuracy: 0.1126\n",
            "Epoch 35/50\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 149.0969 - accuracy: 0.1103\n",
            "Epoch 36/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 149.1703 - accuracy: 0.1115\n",
            "Epoch 37/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 148.2300 - accuracy: 0.1109\n",
            "Epoch 38/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 147.1887 - accuracy: 0.1175\n",
            "Epoch 39/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 148.3849 - accuracy: 0.1120\n",
            "Epoch 40/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 146.2552 - accuracy: 0.1152\n",
            "Epoch 41/50\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 146.8582 - accuracy: 0.1135\n",
            "Epoch 42/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 144.8329 - accuracy: 0.1134\n",
            "Epoch 43/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 145.8323 - accuracy: 0.1094\n",
            "Epoch 44/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 144.0896 - accuracy: 0.1156\n",
            "Epoch 45/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 143.0795 - accuracy: 0.1150\n",
            "Epoch 46/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 143.1975 - accuracy: 0.1124\n",
            "Epoch 47/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 141.4357 - accuracy: 0.1177\n",
            "Epoch 48/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 141.9980 - accuracy: 0.1171\n",
            "Epoch 49/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 141.6352 - accuracy: 0.1177\n",
            "Epoch 50/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 141.2396 - accuracy: 0.1127\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fcf1020f200> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Epoch 1/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 139.5812 - accuracy: 0.0889\n",
            "Epoch 2/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 139.1035 - accuracy: 0.0904\n",
            "Epoch 3/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 138.1498 - accuracy: 0.0917\n",
            "Epoch 4/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 137.8227 - accuracy: 0.0884\n",
            "Epoch 5/50\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 137.9297 - accuracy: 0.0851\n",
            "Epoch 6/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 137.9630 - accuracy: 0.0873\n",
            "Epoch 7/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 137.4833 - accuracy: 0.0907\n",
            "Epoch 8/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 137.6602 - accuracy: 0.0899\n",
            "Epoch 9/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 136.1275 - accuracy: 0.0916\n",
            "Epoch 10/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 135.8294 - accuracy: 0.0904\n",
            "Epoch 11/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 135.5691 - accuracy: 0.0902\n",
            "Epoch 12/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 134.7699 - accuracy: 0.0931\n",
            "Epoch 13/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 135.0031 - accuracy: 0.0930\n",
            "Epoch 14/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 133.7623 - accuracy: 0.0916\n",
            "Epoch 15/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 133.8378 - accuracy: 0.0933\n",
            "Epoch 16/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 133.5953 - accuracy: 0.0932\n",
            "Epoch 17/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 133.1024 - accuracy: 0.0934\n",
            "Epoch 18/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 131.4407 - accuracy: 0.0942\n",
            "Epoch 19/50\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 131.2903 - accuracy: 0.0938\n",
            "Epoch 20/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 130.6373 - accuracy: 0.0984\n",
            "Epoch 21/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 131.5457 - accuracy: 0.0932\n",
            "Epoch 22/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 130.7297 - accuracy: 0.0959\n",
            "Epoch 23/50\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 130.2977 - accuracy: 0.0947\n",
            "Epoch 24/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 130.0262 - accuracy: 0.0955\n",
            "Epoch 25/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 129.6250 - accuracy: 0.0927\n",
            "Epoch 26/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 129.6765 - accuracy: 0.0941\n",
            "Epoch 27/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 129.7614 - accuracy: 0.0947\n",
            "Epoch 28/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 127.6006 - accuracy: 0.0989\n",
            "Epoch 29/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 128.6361 - accuracy: 0.0966\n",
            "Epoch 30/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 127.7127 - accuracy: 0.0961\n",
            "Epoch 31/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 127.9322 - accuracy: 0.0952\n",
            "Epoch 32/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 126.4532 - accuracy: 0.0976\n",
            "Epoch 33/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 126.2021 - accuracy: 0.0941\n",
            "Epoch 34/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 125.7560 - accuracy: 0.0962\n",
            "Epoch 35/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 125.2838 - accuracy: 0.0976\n",
            "Epoch 36/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 124.7069 - accuracy: 0.0988\n",
            "Epoch 37/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 124.8804 - accuracy: 0.0983\n",
            "Epoch 38/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 124.2686 - accuracy: 0.0976\n",
            "Epoch 39/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 123.6147 - accuracy: 0.0996\n",
            "Epoch 40/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 123.9581 - accuracy: 0.0981\n",
            "Epoch 41/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 123.0725 - accuracy: 0.0995\n",
            "Epoch 42/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 122.2938 - accuracy: 0.0985\n",
            "Epoch 43/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 121.4992 - accuracy: 0.1003\n",
            "Epoch 44/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 121.7551 - accuracy: 0.1013\n",
            "Epoch 45/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 120.8907 - accuracy: 0.1006\n",
            "Epoch 46/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 121.3470 - accuracy: 0.0993\n",
            "Epoch 47/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 120.6028 - accuracy: 0.1019\n",
            "Epoch 48/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 120.4905 - accuracy: 0.1006\n",
            "Epoch 49/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 118.9089 - accuracy: 0.1074\n",
            "Epoch 50/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 118.8761 - accuracy: 0.1031\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fcefad150e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Epoch 1/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 148.0895 - accuracy: 0.1250\n",
            "Epoch 2/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 148.8265 - accuracy: 0.1239\n",
            "Epoch 3/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 146.8963 - accuracy: 0.1249\n",
            "Epoch 4/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 145.8903 - accuracy: 0.1234\n",
            "Epoch 5/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 146.4483 - accuracy: 0.1250\n",
            "Epoch 6/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 145.5412 - accuracy: 0.1243\n",
            "Epoch 7/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 145.2040 - accuracy: 0.1254\n",
            "Epoch 8/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 144.9196 - accuracy: 0.1213\n",
            "Epoch 9/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 143.9160 - accuracy: 0.1243\n",
            "Epoch 10/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 143.3064 - accuracy: 0.1243\n",
            "Epoch 11/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 143.2560 - accuracy: 0.1246\n",
            "Epoch 12/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 142.5672 - accuracy: 0.1229\n",
            "Epoch 13/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 141.8855 - accuracy: 0.1268\n",
            "Epoch 14/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 141.8995 - accuracy: 0.1229\n",
            "Epoch 15/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 141.4222 - accuracy: 0.1254\n",
            "Epoch 16/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 140.6080 - accuracy: 0.1227\n",
            "Epoch 17/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 140.0542 - accuracy: 0.1267\n",
            "Epoch 18/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 139.6253 - accuracy: 0.1268\n",
            "Epoch 19/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 139.3839 - accuracy: 0.1242\n",
            "Epoch 20/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 138.9404 - accuracy: 0.1236\n",
            "Epoch 21/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 137.6052 - accuracy: 0.1276\n",
            "Epoch 22/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 137.2335 - accuracy: 0.1282\n",
            "Epoch 23/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 136.3647 - accuracy: 0.1280\n",
            "Epoch 24/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 137.0084 - accuracy: 0.1255\n",
            "Epoch 25/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 137.0811 - accuracy: 0.1220\n",
            "Epoch 26/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 135.4023 - accuracy: 0.1279\n",
            "Epoch 27/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 135.2326 - accuracy: 0.1294\n",
            "Epoch 28/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 135.3888 - accuracy: 0.1233\n",
            "Epoch 29/50\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 135.1439 - accuracy: 0.1247\n",
            "Epoch 30/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 132.4020 - accuracy: 0.1282\n",
            "Epoch 31/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 134.2057 - accuracy: 0.1278\n",
            "Epoch 32/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 133.9030 - accuracy: 0.1245\n",
            "Epoch 33/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 133.4142 - accuracy: 0.1275\n",
            "Epoch 34/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 132.3888 - accuracy: 0.1289\n",
            "Epoch 35/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 132.2718 - accuracy: 0.1277\n",
            "Epoch 36/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 132.4831 - accuracy: 0.1262\n",
            "Epoch 37/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 131.8952 - accuracy: 0.1258\n",
            "Epoch 38/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 131.5526 - accuracy: 0.1261\n",
            "Epoch 39/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 129.7607 - accuracy: 0.1318\n",
            "Epoch 40/50\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 129.9199 - accuracy: 0.1256\n",
            "Epoch 41/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 129.3131 - accuracy: 0.1269\n",
            "Epoch 42/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 128.9957 - accuracy: 0.1276\n",
            "Epoch 43/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 128.0616 - accuracy: 0.1294\n",
            "Epoch 44/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 127.9142 - accuracy: 0.1311\n",
            "Epoch 45/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 128.1016 - accuracy: 0.1283\n",
            "Epoch 46/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 127.0590 - accuracy: 0.1316\n",
            "Epoch 47/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 126.0596 - accuracy: 0.1331\n",
            "Epoch 48/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 125.9969 - accuracy: 0.1311\n",
            "Epoch 49/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 125.7624 - accuracy: 0.1334\n",
            "Epoch 50/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 124.8073 - accuracy: 0.1327\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fceea6f0950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Epoch 1/40\n",
            "7/7 [==============================] - 1s 4ms/step - loss: 2.4196 - accuracy: 0.1767\n",
            "Epoch 2/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 1.6038 - accuracy: 0.5677\n",
            "Epoch 3/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 1.1859 - accuracy: 0.7464\n",
            "Epoch 4/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.9407 - accuracy: 0.8150\n",
            "Epoch 5/40\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.7703 - accuracy: 0.8483\n",
            "Epoch 6/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.6510 - accuracy: 0.8695\n",
            "Epoch 7/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.5860 - accuracy: 0.8811\n",
            "Epoch 8/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.5263 - accuracy: 0.8918\n",
            "Epoch 9/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.4860 - accuracy: 0.8996\n",
            "Epoch 10/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.4453 - accuracy: 0.9057\n",
            "Epoch 11/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.4068 - accuracy: 0.9163\n",
            "Epoch 12/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.3798 - accuracy: 0.9211\n",
            "Epoch 13/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.3655 - accuracy: 0.9228\n",
            "Epoch 14/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.3407 - accuracy: 0.9297\n",
            "Epoch 15/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.3176 - accuracy: 0.9327\n",
            "Epoch 16/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.2967 - accuracy: 0.9371\n",
            "Epoch 17/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.2869 - accuracy: 0.9360\n",
            "Epoch 18/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.2862 - accuracy: 0.9366\n",
            "Epoch 19/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.2608 - accuracy: 0.9437\n",
            "Epoch 20/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.2471 - accuracy: 0.9483\n",
            "Epoch 21/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.2392 - accuracy: 0.9490\n",
            "Epoch 22/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.2395 - accuracy: 0.9482\n",
            "Epoch 23/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.2388 - accuracy: 0.9473\n",
            "Epoch 24/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.2252 - accuracy: 0.9506\n",
            "Epoch 25/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.2167 - accuracy: 0.9518\n",
            "Epoch 26/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.2040 - accuracy: 0.9565\n",
            "Epoch 27/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1928 - accuracy: 0.9588\n",
            "Epoch 28/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1908 - accuracy: 0.9597\n",
            "Epoch 29/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1862 - accuracy: 0.9603\n",
            "Epoch 30/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1819 - accuracy: 0.9610\n",
            "Epoch 31/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1706 - accuracy: 0.9620\n",
            "Epoch 32/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1687 - accuracy: 0.9632\n",
            "Epoch 33/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1579 - accuracy: 0.9690\n",
            "Epoch 34/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1569 - accuracy: 0.9661\n",
            "Epoch 35/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1524 - accuracy: 0.9695\n",
            "Epoch 36/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1464 - accuracy: 0.9696\n",
            "Epoch 37/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1434 - accuracy: 0.9713\n",
            "Epoch 38/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1415 - accuracy: 0.9715\n",
            "Epoch 39/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1339 - accuracy: 0.9710\n",
            "Epoch 40/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1339 - accuracy: 0.9717\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fcf111ec290> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Epoch 1/40\n",
            "7/7 [==============================] - 1s 4ms/step - loss: 2.4575 - accuracy: 0.1748\n",
            "Epoch 2/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 1.6376 - accuracy: 0.5296\n",
            "Epoch 3/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 1.2251 - accuracy: 0.7126\n",
            "Epoch 4/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.9751 - accuracy: 0.7865\n",
            "Epoch 5/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.8029 - accuracy: 0.8300\n",
            "Epoch 6/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.7017 - accuracy: 0.8501\n",
            "Epoch 7/40\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.6105 - accuracy: 0.8770\n",
            "Epoch 8/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.5571 - accuracy: 0.8845\n",
            "Epoch 9/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.5068 - accuracy: 0.8883\n",
            "Epoch 10/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.4691 - accuracy: 0.8975\n",
            "Epoch 11/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.4296 - accuracy: 0.9075\n",
            "Epoch 12/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.4076 - accuracy: 0.9093\n",
            "Epoch 13/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.3776 - accuracy: 0.9131\n",
            "Epoch 14/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.3488 - accuracy: 0.9221\n",
            "Epoch 15/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.3319 - accuracy: 0.9272\n",
            "Epoch 16/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.3230 - accuracy: 0.9264\n",
            "Epoch 17/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.3013 - accuracy: 0.9316\n",
            "Epoch 18/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.2908 - accuracy: 0.9369\n",
            "Epoch 19/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.2767 - accuracy: 0.9390\n",
            "Epoch 20/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.2570 - accuracy: 0.9447\n",
            "Epoch 21/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.2546 - accuracy: 0.9443\n",
            "Epoch 22/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.2401 - accuracy: 0.9474\n",
            "Epoch 23/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.2280 - accuracy: 0.9512\n",
            "Epoch 24/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.2245 - accuracy: 0.9491\n",
            "Epoch 25/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.2122 - accuracy: 0.9548\n",
            "Epoch 26/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.2101 - accuracy: 0.9515\n",
            "Epoch 27/40\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.1995 - accuracy: 0.9569\n",
            "Epoch 28/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.2001 - accuracy: 0.9538\n",
            "Epoch 29/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1843 - accuracy: 0.9608\n",
            "Epoch 30/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1779 - accuracy: 0.9638\n",
            "Epoch 31/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1731 - accuracy: 0.9646\n",
            "Epoch 32/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1668 - accuracy: 0.9648\n",
            "Epoch 33/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1615 - accuracy: 0.9634\n",
            "Epoch 34/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1632 - accuracy: 0.9658\n",
            "Epoch 35/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1595 - accuracy: 0.9648\n",
            "Epoch 36/40\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.1539 - accuracy: 0.9676\n",
            "Epoch 37/40\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.1471 - accuracy: 0.9689\n",
            "Epoch 38/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1445 - accuracy: 0.9686\n",
            "Epoch 39/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1383 - accuracy: 0.9715\n",
            "Epoch 40/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1353 - accuracy: 0.9721\n",
            "WARNING:tensorflow:5 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fcef212f950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Epoch 1/40\n",
            "7/7 [==============================] - 1s 5ms/step - loss: 2.2929 - accuracy: 0.1773\n",
            "Epoch 2/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 1.5959 - accuracy: 0.5558\n",
            "Epoch 3/40\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 1.2340 - accuracy: 0.7035\n",
            "Epoch 4/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 1.0080 - accuracy: 0.7793\n",
            "Epoch 5/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.8556 - accuracy: 0.8118\n",
            "Epoch 6/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.7456 - accuracy: 0.8358\n",
            "Epoch 7/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.6661 - accuracy: 0.8460\n",
            "Epoch 8/40\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.5992 - accuracy: 0.8630\n",
            "Epoch 9/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.5396 - accuracy: 0.8764\n",
            "Epoch 10/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.5038 - accuracy: 0.8848\n",
            "Epoch 11/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.4570 - accuracy: 0.8967\n",
            "Epoch 12/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.4307 - accuracy: 0.9044\n",
            "Epoch 13/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.4144 - accuracy: 0.9048\n",
            "Epoch 14/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.3852 - accuracy: 0.9136\n",
            "Epoch 15/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.3791 - accuracy: 0.9123\n",
            "Epoch 16/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.3566 - accuracy: 0.9170\n",
            "Epoch 17/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.3281 - accuracy: 0.9231\n",
            "Epoch 18/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.3301 - accuracy: 0.9227\n",
            "Epoch 19/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.3073 - accuracy: 0.9247\n",
            "Epoch 20/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.2938 - accuracy: 0.9306\n",
            "Epoch 21/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.2865 - accuracy: 0.9303\n",
            "Epoch 22/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.2722 - accuracy: 0.9361\n",
            "Epoch 23/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.2663 - accuracy: 0.9365\n",
            "Epoch 24/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.2581 - accuracy: 0.9376\n",
            "Epoch 25/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.2454 - accuracy: 0.9447\n",
            "Epoch 26/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.2488 - accuracy: 0.9364\n",
            "Epoch 27/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.2245 - accuracy: 0.9494\n",
            "Epoch 28/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.2291 - accuracy: 0.9460\n",
            "Epoch 29/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.2229 - accuracy: 0.9456\n",
            "Epoch 30/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.2120 - accuracy: 0.9523\n",
            "Epoch 31/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1960 - accuracy: 0.9567\n",
            "Epoch 32/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.2004 - accuracy: 0.9561\n",
            "Epoch 33/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1923 - accuracy: 0.9598\n",
            "Epoch 34/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1886 - accuracy: 0.9594\n",
            "Epoch 35/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1757 - accuracy: 0.9619\n",
            "Epoch 36/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1753 - accuracy: 0.9639\n",
            "Epoch 37/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1774 - accuracy: 0.9603\n",
            "Epoch 38/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1689 - accuracy: 0.9642\n",
            "Epoch 39/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1629 - accuracy: 0.9648\n",
            "Epoch 40/40\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 0.1531 - accuracy: 0.9683\n",
            "Epoch 1/20\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 159.5641 - accuracy: 0.1776\n",
            "Epoch 2/20\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 37.7668 - accuracy: 0.5463\n",
            "Epoch 3/20\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 15.3098 - accuracy: 0.7357\n",
            "Epoch 4/20\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 10.8026 - accuracy: 0.7920\n",
            "Epoch 5/20\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 16.3835 - accuracy: 0.7451\n",
            "Epoch 6/20\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 7.2433 - accuracy: 0.8507\n",
            "Epoch 7/20\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 9.4061 - accuracy: 0.8249\n",
            "Epoch 8/20\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 10.0979 - accuracy: 0.8180\n",
            "Epoch 9/20\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 4.2762 - accuracy: 0.9014\n",
            "Epoch 10/20\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 3.5820 - accuracy: 0.9057\n",
            "Epoch 11/20\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 7.4588 - accuracy: 0.8429\n",
            "Epoch 12/20\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 3.6433 - accuracy: 0.9049\n",
            "Epoch 13/20\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 2.3302 - accuracy: 0.9330\n",
            "Epoch 14/20\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 2.3704 - accuracy: 0.9276\n",
            "Epoch 15/20\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 8.6170 - accuracy: 0.8522\n",
            "Epoch 16/20\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 3.0332 - accuracy: 0.9174\n",
            "Epoch 17/20\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 1.2734 - accuracy: 0.9530\n",
            "Epoch 18/20\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 4.2019 - accuracy: 0.9115\n",
            "Epoch 19/20\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.1045 - accuracy: 0.9604\n",
            "Epoch 20/20\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.8068 - accuracy: 0.9653\n",
            "Epoch 1/20\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 121.3499 - accuracy: 0.1871\n",
            "Epoch 2/20\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 27.2334 - accuracy: 0.5921\n",
            "Epoch 3/20\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 13.2649 - accuracy: 0.7302\n",
            "Epoch 4/20\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 9.8934 - accuracy: 0.7850\n",
            "Epoch 5/20\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 8.1436 - accuracy: 0.8061\n",
            "Epoch 6/20\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 6.8463 - accuracy: 0.8344\n",
            "Epoch 7/20\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 4.9891 - accuracy: 0.8590\n",
            "Epoch 8/20\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 4.8934 - accuracy: 0.8613\n",
            "Epoch 9/20\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 9.1255 - accuracy: 0.8013\n",
            "Epoch 10/20\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 2.7643 - accuracy: 0.9103\n",
            "Epoch 11/20\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 2.3993 - accuracy: 0.9190\n",
            "Epoch 12/20\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 2.5795 - accuracy: 0.9092\n",
            "Epoch 13/20\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 7.7750 - accuracy: 0.8413\n",
            "Epoch 14/20\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.3807 - accuracy: 0.9454\n",
            "Epoch 15/20\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 1.0615 - accuracy: 0.9475\n",
            "Epoch 16/20\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.2432 - accuracy: 0.9439\n",
            "Epoch 17/20\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 8.4266 - accuracy: 0.8340\n",
            "Epoch 18/20\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.9914 - accuracy: 0.9563\n",
            "Epoch 19/20\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.5761 - accuracy: 0.9696\n",
            "Epoch 20/20\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5983 - accuracy: 0.9634\n",
            "Epoch 1/20\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 109.0008 - accuracy: 0.2021\n",
            "Epoch 2/20\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 27.5267 - accuracy: 0.6187\n",
            "Epoch 3/20\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 16.5938 - accuracy: 0.6992\n",
            "Epoch 4/20\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 11.8988 - accuracy: 0.7578\n",
            "Epoch 5/20\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 8.1084 - accuracy: 0.8106\n",
            "Epoch 6/20\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 14.2397 - accuracy: 0.7376\n",
            "Epoch 7/20\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 5.6616 - accuracy: 0.8548\n",
            "Epoch 8/20\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 5.1385 - accuracy: 0.8608\n",
            "Epoch 9/20\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 8.3785 - accuracy: 0.8058\n",
            "Epoch 10/20\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 3.3180 - accuracy: 0.8990\n",
            "Epoch 11/20\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 4.9896 - accuracy: 0.8578\n",
            "Epoch 12/20\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 5.4325 - accuracy: 0.8716\n",
            "Epoch 13/20\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 2.7671 - accuracy: 0.9031\n",
            "Epoch 14/20\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 2.7767 - accuracy: 0.9073\n",
            "Epoch 15/20\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 4.8678 - accuracy: 0.8646\n",
            "Epoch 16/20\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 1.5900 - accuracy: 0.9382\n",
            "Epoch 17/20\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.5393 - accuracy: 0.9372\n",
            "Epoch 18/20\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 6.3818 - accuracy: 0.8448\n",
            "Epoch 19/20\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.5577 - accuracy: 0.9381\n",
            "Epoch 20/20\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7603 - accuracy: 0.9647\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fceea0c9830> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Epoch 1/30\n",
            "7/7 [==============================] - 1s 4ms/step - loss: 175.4162 - accuracy: 0.1823\n",
            "Epoch 2/30\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 38.8055 - accuracy: 0.6151\n",
            "Epoch 3/30\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 24.1775 - accuracy: 0.7309\n",
            "Epoch 4/30\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 18.3831 - accuracy: 0.7913\n",
            "Epoch 5/30\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 15.2174 - accuracy: 0.8258\n",
            "Epoch 6/30\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 12.9225 - accuracy: 0.8425\n",
            "Epoch 7/30\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 12.0512 - accuracy: 0.8585\n",
            "Epoch 8/30\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 10.0337 - accuracy: 0.8672\n",
            "Epoch 9/30\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 10.0684 - accuracy: 0.8735\n",
            "Epoch 10/30\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 9.6159 - accuracy: 0.8758\n",
            "Epoch 11/30\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 8.2334 - accuracy: 0.8814\n",
            "Epoch 12/30\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 7.8509 - accuracy: 0.8898\n",
            "Epoch 13/30\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 7.4520 - accuracy: 0.8886\n",
            "Epoch 14/30\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 6.8043 - accuracy: 0.8955\n",
            "Epoch 15/30\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 6.5699 - accuracy: 0.8953\n",
            "Epoch 16/30\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 5.9839 - accuracy: 0.8974\n",
            "Epoch 17/30\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 6.0904 - accuracy: 0.8975\n",
            "Epoch 18/30\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 5.9401 - accuracy: 0.8941\n",
            "Epoch 19/30\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 5.8225 - accuracy: 0.9039\n",
            "Epoch 20/30\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 5.1545 - accuracy: 0.9062\n",
            "Epoch 21/30\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 5.0538 - accuracy: 0.9084\n",
            "Epoch 22/30\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 4.6485 - accuracy: 0.9096\n",
            "Epoch 23/30\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 4.8704 - accuracy: 0.9123\n",
            "Epoch 24/30\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 4.5594 - accuracy: 0.9131\n",
            "Epoch 25/30\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 4.4119 - accuracy: 0.9117\n",
            "Epoch 26/30\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 4.2226 - accuracy: 0.9168\n",
            "Epoch 27/30\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 3.8182 - accuracy: 0.9200\n",
            "Epoch 28/30\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 3.9008 - accuracy: 0.9188\n",
            "Epoch 29/30\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 3.9124 - accuracy: 0.9180\n",
            "Epoch 30/30\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 3.7530 - accuracy: 0.9214\n",
            "WARNING:tensorflow:5 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fcef4594830> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Epoch 1/30\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 188.7141 - accuracy: 0.1640\n",
            "Epoch 2/30\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 45.3029 - accuracy: 0.5632\n",
            "Epoch 3/30\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 26.0579 - accuracy: 0.7221\n",
            "Epoch 4/30\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 20.1559 - accuracy: 0.7744\n",
            "Epoch 5/30\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 16.1119 - accuracy: 0.8065\n",
            "Epoch 6/30\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 13.1993 - accuracy: 0.8418\n",
            "Epoch 7/30\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 12.1730 - accuracy: 0.8451\n",
            "Epoch 8/30\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 11.3514 - accuracy: 0.8544\n",
            "Epoch 9/30\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 10.1359 - accuracy: 0.8622\n",
            "Epoch 10/30\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 9.5479 - accuracy: 0.8668\n",
            "Epoch 11/30\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 8.9842 - accuracy: 0.8690\n",
            "Epoch 12/30\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 8.1861 - accuracy: 0.8733\n",
            "Epoch 13/30\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 7.5039 - accuracy: 0.8788\n",
            "Epoch 14/30\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 7.5226 - accuracy: 0.8785\n",
            "Epoch 15/30\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 6.5696 - accuracy: 0.8806\n",
            "Epoch 16/30\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 6.2559 - accuracy: 0.8888\n",
            "Epoch 17/30\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 5.9740 - accuracy: 0.8898\n",
            "Epoch 18/30\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 5.9818 - accuracy: 0.8939\n",
            "Epoch 19/30\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 5.3002 - accuracy: 0.8951\n",
            "Epoch 20/30\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 4.9966 - accuracy: 0.8980\n",
            "Epoch 21/30\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 5.1165 - accuracy: 0.8962\n",
            "Epoch 22/30\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 4.7494 - accuracy: 0.9027\n",
            "Epoch 23/30\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 4.8410 - accuracy: 0.9021\n",
            "Epoch 24/30\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 4.4721 - accuracy: 0.9022\n",
            "Epoch 25/30\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 4.3814 - accuracy: 0.9036\n",
            "Epoch 26/30\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 4.2699 - accuracy: 0.9046\n",
            "Epoch 27/30\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 3.9426 - accuracy: 0.9095\n",
            "Epoch 28/30\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 3.7994 - accuracy: 0.9117\n",
            "Epoch 29/30\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 3.4987 - accuracy: 0.9134\n",
            "Epoch 30/30\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 3.6705 - accuracy: 0.9101\n",
            "WARNING:tensorflow:5 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fceea235c20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Epoch 1/30\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 154.8394 - accuracy: 0.2062\n",
            "Epoch 2/30\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 41.8991 - accuracy: 0.5770\n",
            "Epoch 3/30\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 28.2242 - accuracy: 0.6997\n",
            "Epoch 4/30\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 20.6996 - accuracy: 0.7698\n",
            "Epoch 5/30\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 17.3716 - accuracy: 0.8007\n",
            "Epoch 6/30\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 15.2480 - accuracy: 0.8078\n",
            "Epoch 7/30\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 13.1191 - accuracy: 0.8306\n",
            "Epoch 8/30\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 12.6193 - accuracy: 0.8302\n",
            "Epoch 9/30\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 11.7096 - accuracy: 0.8339\n",
            "Epoch 10/30\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 10.6015 - accuracy: 0.8413\n",
            "Epoch 11/30\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 10.0794 - accuracy: 0.8451\n",
            "Epoch 12/30\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 9.4798 - accuracy: 0.8509\n",
            "Epoch 13/30\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 9.0120 - accuracy: 0.8533\n",
            "Epoch 14/30\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 8.1562 - accuracy: 0.8585\n",
            "Epoch 15/30\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 8.0506 - accuracy: 0.8608\n",
            "Epoch 16/30\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 7.8326 - accuracy: 0.8626\n",
            "Epoch 17/30\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 7.6607 - accuracy: 0.8655\n",
            "Epoch 18/30\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 7.4042 - accuracy: 0.8706\n",
            "Epoch 19/30\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 6.8846 - accuracy: 0.8634\n",
            "Epoch 20/30\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 6.2948 - accuracy: 0.8768\n",
            "Epoch 21/30\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 6.1395 - accuracy: 0.8794\n",
            "Epoch 22/30\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 6.0893 - accuracy: 0.8756\n",
            "Epoch 23/30\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 5.6739 - accuracy: 0.8820\n",
            "Epoch 24/30\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 5.3641 - accuracy: 0.8868\n",
            "Epoch 25/30\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 4.8950 - accuracy: 0.8879\n",
            "Epoch 26/30\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 4.8171 - accuracy: 0.8905\n",
            "Epoch 27/30\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 4.8245 - accuracy: 0.8902\n",
            "Epoch 28/30\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 4.7668 - accuracy: 0.8944\n",
            "Epoch 29/30\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 4.5220 - accuracy: 0.8984\n",
            "Epoch 30/30\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 4.4997 - accuracy: 0.8944\n",
            "Epoch 1/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 2.9415 - accuracy: 0.1294\n",
            "Epoch 2/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 1.7411 - accuracy: 0.4088\n",
            "Epoch 3/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.2161 - accuracy: 0.6138\n",
            "Epoch 4/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.9549 - accuracy: 0.7118\n",
            "Epoch 5/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.7946 - accuracy: 0.7714\n",
            "Epoch 6/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6938 - accuracy: 0.8041\n",
            "Epoch 7/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6028 - accuracy: 0.8340\n",
            "Epoch 8/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5547 - accuracy: 0.8503\n",
            "Epoch 9/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5035 - accuracy: 0.8655\n",
            "Epoch 10/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4705 - accuracy: 0.8706\n",
            "Epoch 11/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4435 - accuracy: 0.8782\n",
            "Epoch 12/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.4180 - accuracy: 0.8861\n",
            "Epoch 13/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.3881 - accuracy: 0.8998\n",
            "Epoch 14/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.3755 - accuracy: 0.9021\n",
            "Epoch 15/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.3527 - accuracy: 0.9077\n",
            "Epoch 16/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.3414 - accuracy: 0.9080\n",
            "Epoch 17/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.3235 - accuracy: 0.9131\n",
            "Epoch 18/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.3183 - accuracy: 0.9154\n",
            "Epoch 19/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.3029 - accuracy: 0.9191\n",
            "Epoch 20/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.2892 - accuracy: 0.9222\n",
            "Epoch 21/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.2771 - accuracy: 0.9254\n",
            "Epoch 22/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.2720 - accuracy: 0.9286\n",
            "Epoch 23/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.2548 - accuracy: 0.9360\n",
            "Epoch 24/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.2497 - accuracy: 0.9376\n",
            "Epoch 25/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.2503 - accuracy: 0.9316\n",
            "Epoch 26/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.2407 - accuracy: 0.9354\n",
            "Epoch 27/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.2416 - accuracy: 0.9343\n",
            "Epoch 28/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.2339 - accuracy: 0.9357\n",
            "Epoch 29/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.2315 - accuracy: 0.9388\n",
            "Epoch 30/30\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.2211 - accuracy: 0.9430\n",
            "Epoch 1/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 2.7608 - accuracy: 0.1520\n",
            "Epoch 2/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 1.6510 - accuracy: 0.4634\n",
            "Epoch 3/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.2008 - accuracy: 0.6325\n",
            "Epoch 4/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.9587 - accuracy: 0.7125\n",
            "Epoch 5/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.8075 - accuracy: 0.7651\n",
            "Epoch 6/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7023 - accuracy: 0.8016\n",
            "Epoch 7/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6254 - accuracy: 0.8252\n",
            "Epoch 8/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5740 - accuracy: 0.8422\n",
            "Epoch 9/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5294 - accuracy: 0.8539\n",
            "Epoch 10/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4852 - accuracy: 0.8677\n",
            "Epoch 11/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4550 - accuracy: 0.8755\n",
            "Epoch 12/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4263 - accuracy: 0.8841\n",
            "Epoch 13/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.3926 - accuracy: 0.8911\n",
            "Epoch 14/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.3765 - accuracy: 0.8942\n",
            "Epoch 15/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.3526 - accuracy: 0.9038\n",
            "Epoch 16/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.3516 - accuracy: 0.9042\n",
            "Epoch 17/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.3358 - accuracy: 0.9087\n",
            "Epoch 18/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.3274 - accuracy: 0.9111\n",
            "Epoch 19/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.3141 - accuracy: 0.9144\n",
            "Epoch 20/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.2972 - accuracy: 0.9195\n",
            "Epoch 21/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.2878 - accuracy: 0.9218\n",
            "Epoch 22/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.2699 - accuracy: 0.9263\n",
            "Epoch 23/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2652 - accuracy: 0.9300\n",
            "Epoch 24/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.2557 - accuracy: 0.9350\n",
            "Epoch 25/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.2476 - accuracy: 0.9361\n",
            "Epoch 26/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.2440 - accuracy: 0.9367\n",
            "Epoch 27/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.2242 - accuracy: 0.9427\n",
            "Epoch 28/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.2243 - accuracy: 0.9422\n",
            "Epoch 29/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.2226 - accuracy: 0.9443\n",
            "Epoch 30/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.2251 - accuracy: 0.9392\n",
            "Epoch 1/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 2.6976 - accuracy: 0.1534\n",
            "Epoch 2/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.6783 - accuracy: 0.4196\n",
            "Epoch 3/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.2033 - accuracy: 0.6242\n",
            "Epoch 4/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.9757 - accuracy: 0.7009\n",
            "Epoch 5/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.8423 - accuracy: 0.7500\n",
            "Epoch 6/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7467 - accuracy: 0.7807\n",
            "Epoch 7/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6700 - accuracy: 0.8063\n",
            "Epoch 8/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6193 - accuracy: 0.8196\n",
            "Epoch 9/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5684 - accuracy: 0.8332\n",
            "Epoch 10/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5380 - accuracy: 0.8465\n",
            "Epoch 11/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5057 - accuracy: 0.8575\n",
            "Epoch 12/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.4693 - accuracy: 0.8639\n",
            "Epoch 13/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.4530 - accuracy: 0.8691\n",
            "Epoch 14/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4260 - accuracy: 0.8781\n",
            "Epoch 15/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4137 - accuracy: 0.8839\n",
            "Epoch 16/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.3866 - accuracy: 0.8934\n",
            "Epoch 17/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.3816 - accuracy: 0.8924\n",
            "Epoch 18/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.3658 - accuracy: 0.8981\n",
            "Epoch 19/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.3619 - accuracy: 0.8986\n",
            "Epoch 20/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.3546 - accuracy: 0.8984\n",
            "Epoch 21/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.3466 - accuracy: 0.9046\n",
            "Epoch 22/30\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.3351 - accuracy: 0.9090\n",
            "Epoch 23/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.3231 - accuracy: 0.9113\n",
            "Epoch 24/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.3118 - accuracy: 0.9148\n",
            "Epoch 25/30\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.2978 - accuracy: 0.9209\n",
            "Epoch 26/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.2957 - accuracy: 0.9207\n",
            "Epoch 27/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.2832 - accuracy: 0.9258\n",
            "Epoch 28/30\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.2725 - accuracy: 0.9298\n",
            "Epoch 29/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.2645 - accuracy: 0.9304\n",
            "Epoch 30/30\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.2660 - accuracy: 0.9293\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fcefc6db170> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Epoch 1/10\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 152.0821 - accuracy: 0.0653\n",
            "Epoch 2/10\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 48.9457 - accuracy: 0.3424\n",
            "Epoch 3/10\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 26.0693 - accuracy: 0.5466\n",
            "Epoch 4/10\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 17.2764 - accuracy: 0.6589\n",
            "Epoch 5/10\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 13.6294 - accuracy: 0.7188\n",
            "Epoch 6/10\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 11.0173 - accuracy: 0.7662\n",
            "Epoch 7/10\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 8.9817 - accuracy: 0.7996\n",
            "Epoch 8/10\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 8.3021 - accuracy: 0.8179\n",
            "Epoch 9/10\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 7.2862 - accuracy: 0.8342\n",
            "Epoch 10/10\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 6.5141 - accuracy: 0.8471\n",
            "WARNING:tensorflow:5 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fcef6869e60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Epoch 1/10\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 97.3348 - accuracy: 0.1437\n",
            "Epoch 2/10\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 31.7618 - accuracy: 0.4913\n",
            "Epoch 3/10\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 18.0335 - accuracy: 0.6534\n",
            "Epoch 4/10\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 12.9148 - accuracy: 0.7327\n",
            "Epoch 5/10\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 10.0206 - accuracy: 0.7879\n",
            "Epoch 6/10\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 8.7034 - accuracy: 0.8121\n",
            "Epoch 7/10\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 7.3101 - accuracy: 0.8332\n",
            "Epoch 8/10\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 6.5173 - accuracy: 0.8446\n",
            "Epoch 9/10\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 6.2224 - accuracy: 0.8540\n",
            "Epoch 10/10\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 5.6555 - accuracy: 0.8618\n",
            "WARNING:tensorflow:6 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fcef206bd40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Epoch 1/10\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 127.9658 - accuracy: 0.1026\n",
            "Epoch 2/10\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 45.9333 - accuracy: 0.3857\n",
            "Epoch 3/10\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 24.2631 - accuracy: 0.5749\n",
            "Epoch 4/10\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 17.6455 - accuracy: 0.6726\n",
            "Epoch 5/10\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 13.4284 - accuracy: 0.7333\n",
            "Epoch 6/10\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 11.7657 - accuracy: 0.7647\n",
            "Epoch 7/10\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 10.0792 - accuracy: 0.7884\n",
            "Epoch 8/10\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 8.8050 - accuracy: 0.8044\n",
            "Epoch 9/10\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 8.1331 - accuracy: 0.8146\n",
            "Epoch 10/10\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 7.6163 - accuracy: 0.8255\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fceea301b00> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Epoch 1/10\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 2.3474 - accuracy: 0.1640\n",
            "Epoch 2/10\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 1.7904 - accuracy: 0.4453\n",
            "Epoch 3/10\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 1.5145 - accuracy: 0.5995\n",
            "Epoch 4/10\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 1.3267 - accuracy: 0.6803\n",
            "Epoch 5/10\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.1929 - accuracy: 0.7322\n",
            "Epoch 6/10\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.0823 - accuracy: 0.7682\n",
            "Epoch 7/10\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.9942 - accuracy: 0.7935\n",
            "Epoch 8/10\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.9236 - accuracy: 0.8148\n",
            "Epoch 9/10\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.8677 - accuracy: 0.8255\n",
            "Epoch 10/10\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.8185 - accuracy: 0.8383\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fceea396e60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Epoch 1/10\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 2.3730 - accuracy: 0.1374\n",
            "Epoch 2/10\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 1.8758 - accuracy: 0.4001\n",
            "Epoch 3/10\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.6087 - accuracy: 0.5673\n",
            "Epoch 4/10\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 1.4238 - accuracy: 0.6520\n",
            "Epoch 5/10\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.2821 - accuracy: 0.7149\n",
            "Epoch 6/10\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.1799 - accuracy: 0.7476\n",
            "Epoch 7/10\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 1.0878 - accuracy: 0.7763\n",
            "Epoch 8/10\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.0110 - accuracy: 0.7876\n",
            "Epoch 9/10\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.9463 - accuracy: 0.8040\n",
            "Epoch 10/10\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.8878 - accuracy: 0.8201\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fceea193710> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Epoch 1/10\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 2.5142 - accuracy: 0.1209\n",
            "Epoch 2/10\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 1.9515 - accuracy: 0.3436\n",
            "Epoch 3/10\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.6810 - accuracy: 0.5233\n",
            "Epoch 4/10\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.4954 - accuracy: 0.6185\n",
            "Epoch 5/10\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 1.3480 - accuracy: 0.6795\n",
            "Epoch 6/10\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.2468 - accuracy: 0.7132\n",
            "Epoch 7/10\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.1501 - accuracy: 0.7423\n",
            "Epoch 8/10\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.0861 - accuracy: 0.7581\n",
            "Epoch 9/10\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 1.0217 - accuracy: 0.7798\n",
            "Epoch 10/10\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.9604 - accuracy: 0.7955\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fcf1115b560> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Epoch 1/50\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 5254.5856 - accuracy: 0.1039\n",
            "Epoch 2/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 471.1200 - accuracy: 0.1606\n",
            "Epoch 3/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 506.2583 - accuracy: 0.1574\n",
            "Epoch 4/50\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 282.8305 - accuracy: 0.1259\n",
            "Epoch 5/50\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 210.3687 - accuracy: 0.1720\n",
            "Epoch 6/50\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 220.0547 - accuracy: 0.1673\n",
            "Epoch 7/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 365.7560 - accuracy: 0.1523\n",
            "Epoch 8/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 238.7545 - accuracy: 0.1458\n",
            "Epoch 9/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 240.4636 - accuracy: 0.1297\n",
            "Epoch 10/50\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 234.7504 - accuracy: 0.1019\n",
            "Epoch 11/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 233.3081 - accuracy: 0.1010\n",
            "Epoch 12/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 229.3571 - accuracy: 0.1013\n",
            "Epoch 13/50\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 229.2939 - accuracy: 0.1027\n",
            "Epoch 14/50\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 225.9071 - accuracy: 0.1010\n",
            "Epoch 15/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 223.4612 - accuracy: 0.0995\n",
            "Epoch 16/50\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 222.4156 - accuracy: 0.1000\n",
            "Epoch 17/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 220.3254 - accuracy: 0.1009\n",
            "Epoch 18/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 217.4847 - accuracy: 0.1039\n",
            "Epoch 19/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 215.4972 - accuracy: 0.1022\n",
            "Epoch 20/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 213.3779 - accuracy: 0.0976\n",
            "Epoch 21/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 212.5505 - accuracy: 0.0969\n",
            "Epoch 22/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 209.5364 - accuracy: 0.1024\n",
            "Epoch 23/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 207.0316 - accuracy: 0.1068\n",
            "Epoch 24/50\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 206.3631 - accuracy: 0.1001\n",
            "Epoch 25/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 203.2139 - accuracy: 0.1034\n",
            "Epoch 26/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 201.2753 - accuracy: 0.1029\n",
            "Epoch 27/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 199.4279 - accuracy: 0.1016\n",
            "Epoch 28/50\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 197.6887 - accuracy: 0.0978\n",
            "Epoch 29/50\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 195.8139 - accuracy: 0.0989\n",
            "Epoch 30/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 193.0670 - accuracy: 0.1030\n",
            "Epoch 31/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 191.0498 - accuracy: 0.1040\n",
            "Epoch 32/50\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 188.8009 - accuracy: 0.1039\n",
            "Epoch 33/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 187.2554 - accuracy: 0.0989\n",
            "Epoch 34/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 184.7002 - accuracy: 0.1009\n",
            "Epoch 35/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 182.7211 - accuracy: 0.1041\n",
            "Epoch 36/50\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 180.6515 - accuracy: 0.0986\n",
            "Epoch 37/50\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 179.1623 - accuracy: 0.0985\n",
            "Epoch 38/50\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 176.9297 - accuracy: 0.0997\n",
            "Epoch 39/50\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 174.8865 - accuracy: 0.1025\n",
            "Epoch 40/50\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 172.4934 - accuracy: 0.1031\n",
            "Epoch 41/50\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 170.2936 - accuracy: 0.1011\n",
            "Epoch 42/50\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 168.4052 - accuracy: 0.1000\n",
            "Epoch 43/50\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 166.0712 - accuracy: 0.0953\n",
            "Epoch 44/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 164.2104 - accuracy: 0.1019\n",
            "Epoch 45/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 161.8334 - accuracy: 0.1026\n",
            "Epoch 46/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 160.8727 - accuracy: 0.1012\n",
            "Epoch 47/50\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 157.6681 - accuracy: 0.1033\n",
            "Epoch 48/50\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 156.4172 - accuracy: 0.1009\n",
            "Epoch 49/50\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 154.2168 - accuracy: 0.1031\n",
            "Epoch 50/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 151.8849 - accuracy: 0.0966\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fcf1126ff80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Epoch 1/50\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 1707.2140 - accuracy: 0.1306\n",
            "Epoch 2/50\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 3170.6917 - accuracy: 0.2566\n",
            "Epoch 3/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 184.3910 - accuracy: 0.2417\n",
            "Epoch 4/50\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 385.6633 - accuracy: 0.1881\n",
            "Epoch 5/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 63.7584 - accuracy: 0.1704\n",
            "Epoch 6/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 62.8817 - accuracy: 0.1458\n",
            "Epoch 7/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 58.3229 - accuracy: 0.1837\n",
            "Epoch 8/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 51.3910 - accuracy: 0.2251\n",
            "Epoch 9/50\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 50.7195 - accuracy: 0.2104\n",
            "Epoch 10/50\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 47.4380 - accuracy: 0.2104\n",
            "Epoch 11/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 45.1246 - accuracy: 0.2133\n",
            "Epoch 12/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 43.0912 - accuracy: 0.2136\n",
            "Epoch 13/50\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 41.7955 - accuracy: 0.2108\n",
            "Epoch 14/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 40.0694 - accuracy: 0.2121\n",
            "Epoch 15/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 38.6992 - accuracy: 0.2146\n",
            "Epoch 16/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 37.6796 - accuracy: 0.2127\n",
            "Epoch 17/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 36.7233 - accuracy: 0.2151\n",
            "Epoch 18/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 35.4738 - accuracy: 0.2139\n",
            "Epoch 19/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 35.1742 - accuracy: 0.2115\n",
            "Epoch 20/50\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 34.2666 - accuracy: 0.2067\n",
            "Epoch 21/50\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 33.0306 - accuracy: 0.2090\n",
            "Epoch 22/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 31.9836 - accuracy: 0.2155\n",
            "Epoch 23/50\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 31.4261 - accuracy: 0.2129\n",
            "Epoch 24/50\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 29.6646 - accuracy: 0.2335\n",
            "Epoch 25/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 29.6428 - accuracy: 0.2385\n",
            "Epoch 26/50\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 28.8091 - accuracy: 0.2062\n",
            "Epoch 27/50\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 27.8723 - accuracy: 0.2127\n",
            "Epoch 28/50\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 26.6271 - accuracy: 0.2199\n",
            "Epoch 29/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 26.5206 - accuracy: 0.2120\n",
            "Epoch 30/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 25.3481 - accuracy: 0.2114\n",
            "Epoch 31/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 24.9023 - accuracy: 0.2142\n",
            "Epoch 32/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 24.1071 - accuracy: 0.2146\n",
            "Epoch 33/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 23.6048 - accuracy: 0.2125\n",
            "Epoch 34/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 23.1529 - accuracy: 0.2151\n",
            "Epoch 35/50\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 22.6371 - accuracy: 0.2093\n",
            "Epoch 36/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 22.1061 - accuracy: 0.2123\n",
            "Epoch 37/50\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 21.7119 - accuracy: 0.2107\n",
            "Epoch 38/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 20.8118 - accuracy: 0.2105\n",
            "Epoch 39/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 20.2847 - accuracy: 0.2131\n",
            "Epoch 40/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 19.7321 - accuracy: 0.2095\n",
            "Epoch 41/50\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 19.3039 - accuracy: 0.2128\n",
            "Epoch 42/50\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 18.7189 - accuracy: 0.2180\n",
            "Epoch 43/50\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 18.3135 - accuracy: 0.2196\n",
            "Epoch 44/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 17.7452 - accuracy: 0.2190\n",
            "Epoch 45/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 17.4277 - accuracy: 0.2198\n",
            "Epoch 46/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 17.0079 - accuracy: 0.2169\n",
            "Epoch 47/50\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 16.7411 - accuracy: 0.2136\n",
            "Epoch 48/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 16.2645 - accuracy: 0.2198\n",
            "Epoch 49/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 15.8506 - accuracy: 0.2214\n",
            "Epoch 50/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 15.5309 - accuracy: 0.2213\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fcf110c59e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Epoch 1/50\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 2027.9016 - accuracy: 0.1237\n",
            "Epoch 2/50\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 425.8932 - accuracy: 0.1789\n",
            "Epoch 3/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 136.7766 - accuracy: 0.1416\n",
            "Epoch 4/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 172.0312 - accuracy: 0.1661\n",
            "Epoch 5/50\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 184.0734 - accuracy: 0.1147\n",
            "Epoch 6/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 143.6434 - accuracy: 0.1033\n",
            "Epoch 7/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 138.0611 - accuracy: 0.1039\n",
            "Epoch 8/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 136.8402 - accuracy: 0.0985\n",
            "Epoch 9/50\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 130.7850 - accuracy: 0.1023\n",
            "Epoch 10/50\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 126.0412 - accuracy: 0.1229\n",
            "Epoch 11/50\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 123.3143 - accuracy: 0.0982\n",
            "Epoch 12/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 119.0419 - accuracy: 0.0955\n",
            "Epoch 13/50\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 114.1821 - accuracy: 0.0981\n",
            "Epoch 14/50\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 109.3867 - accuracy: 0.1001\n",
            "Epoch 15/50\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 105.1092 - accuracy: 0.0971\n",
            "Epoch 16/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 100.3531 - accuracy: 0.0967\n",
            "Epoch 17/50\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 95.2572 - accuracy: 0.0982\n",
            "Epoch 18/50\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 90.4968 - accuracy: 0.0995\n",
            "Epoch 19/50\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 86.3120 - accuracy: 0.0966\n",
            "Epoch 20/50\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 81.4808 - accuracy: 0.0984\n",
            "Epoch 21/50\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 76.7971 - accuracy: 0.0985\n",
            "Epoch 22/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 73.8207 - accuracy: 0.0991\n",
            "Epoch 23/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 71.4930 - accuracy: 0.0989\n",
            "Epoch 24/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 69.5038 - accuracy: 0.1002\n",
            "Epoch 25/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 67.6601 - accuracy: 0.0972\n",
            "Epoch 26/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 65.5418 - accuracy: 0.0980\n",
            "Epoch 27/50\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 63.2959 - accuracy: 0.0988\n",
            "Epoch 28/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 60.9849 - accuracy: 0.1007\n",
            "Epoch 29/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 59.2794 - accuracy: 0.0990\n",
            "Epoch 30/50\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 56.9547 - accuracy: 0.0971\n",
            "Epoch 31/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 54.8916 - accuracy: 0.1009\n",
            "Epoch 32/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 52.6382 - accuracy: 0.1016\n",
            "Epoch 33/50\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 50.5766 - accuracy: 0.1017\n",
            "Epoch 34/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 48.6108 - accuracy: 0.0998\n",
            "Epoch 35/50\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 46.7099 - accuracy: 0.0994\n",
            "Epoch 36/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 44.4449 - accuracy: 0.1006\n",
            "Epoch 37/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 42.5432 - accuracy: 0.1022\n",
            "Epoch 38/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 40.3557 - accuracy: 0.0994\n",
            "Epoch 39/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 38.3590 - accuracy: 0.1020\n",
            "Epoch 40/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 36.2292 - accuracy: 0.0985\n",
            "Epoch 41/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 34.2666 - accuracy: 0.0972\n",
            "Epoch 42/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 32.1146 - accuracy: 0.0999\n",
            "Epoch 43/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 30.0947 - accuracy: 0.0984\n",
            "Epoch 44/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 27.9695 - accuracy: 0.0987\n",
            "Epoch 45/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 26.0123 - accuracy: 0.1009\n",
            "Epoch 46/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 23.8254 - accuracy: 0.0995\n",
            "Epoch 47/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 22.1117 - accuracy: 0.0976\n",
            "Epoch 48/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 20.7846 - accuracy: 0.0989\n",
            "Epoch 49/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 19.4003 - accuracy: 0.0996\n",
            "Epoch 50/50\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 18.3688 - accuracy: 0.1005\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fcf11031cb0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Epoch 1/40\n",
            "10/10 [==============================] - 1s 4ms/step - loss: 2.1929 - accuracy: 0.2459\n",
            "Epoch 2/40\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 1.3646 - accuracy: 0.6608\n",
            "Epoch 3/40\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.9581 - accuracy: 0.8003\n",
            "Epoch 4/40\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.7493 - accuracy: 0.8442\n",
            "Epoch 5/40\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.6254 - accuracy: 0.8665\n",
            "Epoch 6/40\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.5484 - accuracy: 0.8788\n",
            "Epoch 7/40\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.4819 - accuracy: 0.8944\n",
            "Epoch 8/40\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.4321 - accuracy: 0.9007\n",
            "Epoch 9/40\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.4019 - accuracy: 0.9086\n",
            "Epoch 10/40\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.3737 - accuracy: 0.9114\n",
            "Epoch 11/40\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.3411 - accuracy: 0.9196\n",
            "Epoch 12/40\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.3257 - accuracy: 0.9226\n",
            "Epoch 13/40\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.3092 - accuracy: 0.9261\n",
            "Epoch 14/40\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.2881 - accuracy: 0.9308\n",
            "Epoch 15/40\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.2745 - accuracy: 0.9339\n",
            "Epoch 16/40\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.2576 - accuracy: 0.9386\n",
            "Epoch 17/40\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.2524 - accuracy: 0.9389\n",
            "Epoch 18/40\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.2487 - accuracy: 0.9371\n",
            "Epoch 19/40\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.2325 - accuracy: 0.9452\n",
            "Epoch 20/40\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.2210 - accuracy: 0.9477\n",
            "Epoch 21/40\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.2198 - accuracy: 0.9472\n",
            "Epoch 22/40\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.2110 - accuracy: 0.9464\n",
            "Epoch 23/40\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.2022 - accuracy: 0.9510\n",
            "Epoch 24/40\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.1926 - accuracy: 0.9536\n",
            "Epoch 25/40\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.1876 - accuracy: 0.9547\n",
            "Epoch 26/40\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.1820 - accuracy: 0.9569\n",
            "Epoch 27/40\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.1791 - accuracy: 0.9563\n",
            "Epoch 28/40\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.1726 - accuracy: 0.9590\n",
            "Epoch 29/40\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.1644 - accuracy: 0.9638\n",
            "Epoch 30/40\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.1657 - accuracy: 0.9625\n",
            "Epoch 31/40\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.1549 - accuracy: 0.9643\n",
            "Epoch 32/40\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.1554 - accuracy: 0.9633\n",
            "Epoch 33/40\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.1543 - accuracy: 0.9654\n",
            "Epoch 34/40\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.1446 - accuracy: 0.9674\n",
            "Epoch 35/40\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.1459 - accuracy: 0.9674\n",
            "Epoch 36/40\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.1425 - accuracy: 0.9680\n",
            "Epoch 37/40\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.1351 - accuracy: 0.9678\n",
            "Epoch 38/40\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.1278 - accuracy: 0.9715\n",
            "Epoch 39/40\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.1313 - accuracy: 0.9677\n",
            "Epoch 40/40\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.1302 - accuracy: 0.9697\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomizedSearchCV(cv=3, error_score=nan,\n",
              "                   estimator=<tensorflow.python.keras.wrappers.scikit_learn.KerasClassifier object at 0x7fcef9cadd90>,\n",
              "                   iid='deprecated', n_iter=10, n_jobs=1,\n",
              "                   param_distributions={'activation': ['relu', 'elu', 'tanh',\n",
              "                                                       'sigmoid',\n",
              "                                                       'hard_sigmoid',\n",
              "                                                       'linear'],\n",
              "                                        'batch_size': [1024, 2048],\n",
              "                                        'epochs': [10, 20, 30, 40, 50],\n",
              "                                        'optimizer': ['SGD', 'RMSprop',\n",
              "                                                      'Adagrad', 'Adadelta',\n",
              "                                                      'Adam', 'Adamax',\n",
              "                                                      'Nadam']},\n",
              "                   pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
              "                   return_train_score=False, scoring='accuracy', verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DC2ptymNxM50",
        "outputId": "e40dce3e-a7df-4a6f-c978-f17c6780dd57"
      },
      "source": [
        "# Show results\n",
        "print(\"Best: %f using %s\" % (random_search.best_score_, random_search.best_params_))\n",
        "means = random_search.cv_results_['mean_test_score']\n",
        "stds = random_search.cv_results_['std_test_score']\n",
        "params = random_search.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best: 0.901202 using {'optimizer': 'Nadam', 'epochs': 40, 'batch_size': 1024, 'activation': 'hard_sigmoid'}\n",
            "0.800303 (0.023814) with: {'optimizer': 'Adamax', 'epochs': 20, 'batch_size': 2048, 'activation': 'tanh'}\n",
            "0.878101 (0.014063) with: {'optimizer': 'RMSprop', 'epochs': 20, 'batch_size': 2048, 'activation': 'relu'}\n",
            "0.119099 (0.007864) with: {'optimizer': 'Adadelta', 'epochs': 50, 'batch_size': 2048, 'activation': 'elu'}\n",
            "0.901202 (0.017543) with: {'optimizer': 'Nadam', 'epochs': 40, 'batch_size': 1024, 'activation': 'hard_sigmoid'}\n",
            "0.874603 (0.027327) with: {'optimizer': 'RMSprop', 'epochs': 20, 'batch_size': 2048, 'activation': 'elu'}\n",
            "0.841904 (0.026462) with: {'optimizer': 'Adamax', 'epochs': 30, 'batch_size': 1024, 'activation': 'linear'}\n",
            "0.878203 (0.021574) with: {'optimizer': 'Adam', 'epochs': 30, 'batch_size': 2048, 'activation': 'tanh'}\n",
            "0.802506 (0.042875) with: {'optimizer': 'Adamax', 'epochs': 10, 'batch_size': 2048, 'activation': 'relu'}\n",
            "0.785004 (0.028655) with: {'optimizer': 'Adamax', 'epochs': 10, 'batch_size': 2048, 'activation': 'hard_sigmoid'}\n",
            "0.138703 (0.053736) with: {'optimizer': 'SGD', 'epochs': 50, 'batch_size': 2048, 'activation': 'elu'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJD7tUiv74y0"
      },
      "source": [
        "# Choose best Optimized value for the parameters previously defined\n",
        "a=random_search.best_params_\n",
        "epo=a.get('epochs')\n",
        "act=a.get('activation')\n",
        "bat_sz=a.get('batch_size')\n",
        "opt=a.get('optimizer')"
      ],
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QOSiXfXj-0GW",
        "outputId": "7c36a0ad-4a59-454f-ddf3-a989ae9a5650"
      },
      "source": [
        "# Create a simple Model with the best Optimized values\n",
        "model=Sequential()\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128,activation=act))\n",
        "model.add(Dense(10,activation='softmax'))\n",
        "model.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(x_train, y_train, epochs=epo)\n",
        "\n",
        "pd.DataFrame(model.history.history).plot()\n",
        "plt.show()\n",
        "\n",
        "from matplotlib import pyplot\n",
        "\n",
        "# plot loss during training\n",
        "pyplot.subplot(211)\n",
        "pyplot.title('Loss')\n",
        "pyplot.plot(model.history.history['loss'], label='train')\n",
        "#pyplot.plot(model.history.history['val_loss'], label='test')\n",
        "pyplot.legend()\n",
        "\n",
        "# plot accuracy during training\n",
        "pyplot.subplot(212)\n",
        "pyplot.title('Accuracy')\n",
        "pyplot.plot(model.history.history['accuracy'], label='train')\n",
        "#pyplot.plot(model.history.history['val_accuracy'], label='test')\n",
        "pyplot.legend()\n",
        "pyplot.show()"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.8516 - accuracy: 0.7637\n",
            "Epoch 2/40\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.4040 - accuracy: 0.8850\n",
            "Epoch 3/40\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3662 - accuracy: 0.8915\n",
            "Epoch 4/40\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3317 - accuracy: 0.9016\n",
            "Epoch 5/40\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3159 - accuracy: 0.9052\n",
            "Epoch 6/40\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3081 - accuracy: 0.9078\n",
            "Epoch 7/40\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2969 - accuracy: 0.9097\n",
            "Epoch 8/40\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2772 - accuracy: 0.9168\n",
            "Epoch 9/40\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2975 - accuracy: 0.9097\n",
            "Epoch 10/40\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2740 - accuracy: 0.9171\n",
            "Epoch 11/40\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2667 - accuracy: 0.9192\n",
            "Epoch 12/40\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2700 - accuracy: 0.9173\n",
            "Epoch 13/40\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2638 - accuracy: 0.9194\n",
            "Epoch 14/40\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2565 - accuracy: 0.9234\n",
            "Epoch 15/40\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2503 - accuracy: 0.9223\n",
            "Epoch 16/40\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2419 - accuracy: 0.9271\n",
            "Epoch 17/40\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2537 - accuracy: 0.9228\n",
            "Epoch 18/40\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2314 - accuracy: 0.9298\n",
            "Epoch 19/40\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2525 - accuracy: 0.9231\n",
            "Epoch 20/40\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2389 - accuracy: 0.9272\n",
            "Epoch 21/40\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2474 - accuracy: 0.9243\n",
            "Epoch 22/40\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2478 - accuracy: 0.9250\n",
            "Epoch 23/40\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2289 - accuracy: 0.9311\n",
            "Epoch 24/40\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2199 - accuracy: 0.9336\n",
            "Epoch 25/40\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2283 - accuracy: 0.9308\n",
            "Epoch 26/40\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2264 - accuracy: 0.9302\n",
            "Epoch 27/40\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2209 - accuracy: 0.9326\n",
            "Epoch 28/40\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2158 - accuracy: 0.9368\n",
            "Epoch 29/40\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2171 - accuracy: 0.9353\n",
            "Epoch 30/40\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2128 - accuracy: 0.9374\n",
            "Epoch 31/40\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2083 - accuracy: 0.9385\n",
            "Epoch 32/40\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2016 - accuracy: 0.9399\n",
            "Epoch 33/40\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2037 - accuracy: 0.9399\n",
            "Epoch 34/40\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2080 - accuracy: 0.9376\n",
            "Epoch 35/40\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1969 - accuracy: 0.9406\n",
            "Epoch 36/40\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2070 - accuracy: 0.9380\n",
            "Epoch 37/40\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2081 - accuracy: 0.9382\n",
            "Epoch 38/40\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2065 - accuracy: 0.9388\n",
            "Epoch 39/40\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1998 - accuracy: 0.9387\n",
            "Epoch 40/40\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1956 - accuracy: 0.9407\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCDw0Cw7DKq8",
        "outputId": "ea6e8378-085a-4552-c225-85b91df64881"
      },
      "source": [
        "# Compute Validation Loss and Accuracy\n",
        "val_loss,val_acc=model.evaluate(x_test,y_test)\n",
        "print(val_loss,val_acc)"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 0.2130 - accuracy: 0.9338\n",
            "0.21297281980514526 0.9337999820709229\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PUpyhSQdGtN-",
        "outputId": "0864d3e8-6aae-4b68-df77-119178e7b766"
      },
      "source": [
        "# Save the model for future use\n",
        "model.save(\"model_name.model\")"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: model_name.model/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dfeKkrPGxRW"
      },
      "source": [
        "# To use this model\n",
        "new_model_testing=tf.keras.models.load_model(\"model_name.model\")"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kcEVdm5G1hD",
        "outputId": "19847ff8-a0a6-4c12-a3a4-f296aafe0ade"
      },
      "source": [
        "# Make prediction using  model\n",
        "prediction=model.predict(x_test)"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fcef57960e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0-w3NZ8G4dq",
        "outputId": "6aad34ef-bbf6-431d-8395-18b7c6272aae"
      },
      "source": [
        "# Print the list of predictions\n",
        "import numpy as np\n",
        "# this list gives all predictions made\n",
        "list2=[]\n",
        "\n",
        "for i in prediction:\n",
        "    list2.append(np.argmax(i))\n",
        "    \n",
        "print(list2)"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[7, 2, 1, 0, 4, 1, 4, 9, 4, 9, 0, 6, 9, 0, 1, 5, 9, 7, 8, 4, 9, 6, 6, 5, 4, 0, 7, 4, 0, 1, 3, 1, 3, 4, 7, 2, 7, 1, 2, 1, 1, 7, 4, 2, 3, 5, 1, 2, 4, 4, 6, 3, 5, 5, 6, 0, 4, 1, 9, 5, 7, 8, 9, 3, 7, 4, 6, 4, 3, 0, 7, 0, 2, 9, 1, 7, 3, 2, 9, 7, 7, 6, 2, 7, 8, 4, 7, 3, 6, 1, 3, 6, 9, 3, 1, 4, 1, 7, 6, 9, 6, 0, 5, 4, 9, 9, 2, 1, 9, 4, 8, 7, 3, 9, 7, 4, 4, 4, 9, 7, 3, 4, 7, 6, 7, 9, 0, 5, 8, 5, 6, 6, 5, 7, 8, 1, 0, 1, 6, 4, 6, 7, 3, 1, 7, 1, 8, 2, 0, 2, 9, 9, 5, 5, 1, 5, 6, 0, 3, 9, 4, 6, 5, 4, 6, 5, 4, 5, 1, 4, 4, 7, 2, 3, 2, 7, 1, 8, 1, 8, 1, 8, 5, 0, 8, 9, 2, 5, 0, 1, 1, 1, 0, 9, 0, 3, 1, 6, 4, 2, 3, 6, 1, 1, 1, 3, 9, 5, 2, 9, 4, 5, 9, 3, 9, 0, 3, 5, 5, 5, 7, 2, 2, 7, 1, 2, 8, 4, 1, 7, 3, 3, 8, 8, 7, 9, 2, 2, 4, 1, 5, 3, 8, 7, 2, 3, 0, 2, 4, 2, 9, 1, 9, 5, 7, 7, 2, 8, 2, 0, 8, 5, 7, 7, 9, 1, 8, 1, 8, 0, 3, 0, 1, 9, 9, 4, 1, 8, 2, 1, 2, 9, 7, 5, 9, 2, 6, 4, 1, 5, 4, 2, 9, 2, 0, 9, 0, 0, 2, 8, 4, 7, 1, 2, 4, 0, 2, 9, 4, 3, 3, 0, 0, 3, 1, 9, 6, 5, 3, 5, 7, 7, 9, 3, 0, 4, 2, 0, 7, 1, 1, 2, 1, 5, 3, 3, 9, 7, 8, 6, 3, 7, 1, 3, 8, 1, 0, 5, 1, 3, 1, 5, 0, 6, 1, 8, 5, 1, 7, 9, 4, 6, 7, 2, 5, 0, 6, 5, 6, 3, 7, 2, 0, 8, 8, 5, 4, 1, 1, 4, 0, 7, 3, 7, 6, 1, 6, 2, 1, 9, 2, 8, 6, 1, 9, 5, 2, 5, 4, 4, 2, 8, 3, 9, 2, 4, 5, 0, 3, 1, 7, 7, 5, 7, 9, 7, 1, 9, 2, 1, 4, 2, 9, 2, 0, 4, 9, 1, 4, 8, 1, 8, 4, 5, 9, 8, 8, 3, 7, 6, 0, 0, 3, 0, 8, 0, 6, 4, 8, 3, 3, 3, 2, 3, 9, 1, 2, 6, 8, 0, 5, 6, 6, 6, 3, 8, 8, 2, 9, 5, 8, 9, 6, 1, 8, 4, 1, 2, 5, 3, 1, 9, 7, 5, 4, 0, 8, 9, 9, 1, 0, 5, 2, 3, 7, 2, 9, 4, 0, 6, 3, 9, 3, 2, 1, 3, 1, 5, 6, 5, 7, 4, 2, 2, 6, 3, 2, 6, 5, 4, 8, 9, 7, 1, 3, 0, 3, 8, 3, 1, 9, 3, 4, 4, 6, 4, 2, 1, 8, 2, 5, 4, 4, 8, 4, 0, 0, 2, 3, 2, 7, 7, 0, 8, 7, 4, 4, 7, 9, 6, 9, 0, 9, 8, 0, 9, 6, 0, 6, 3, 5, 4, 9, 3, 3, 9, 3, 3, 3, 7, 8, 0, 2, 2, 1, 7, 0, 6, 5, 4, 3, 3, 0, 9, 6, 3, 8, 0, 9, 9, 6, 8, 6, 9, 5, 7, 8, 6, 0, 2, 6, 0, 2, 8, 3, 1, 9, 7, 5, 2, 0, 8, 4, 6, 2, 6, 7, 9, 9, 2, 9, 8, 2, 2, 9, 2, 7, 3, 5, 9, 1, 8, 0, 2, 0, 5, 6, 1, 3, 7, 6, 7, 1, 2, 5, 8, 0, 3, 7, 1, 4, 0, 9, 1, 8, 6, 7, 7, 4, 3, 4, 9, 1, 9, 5, 1, 7, 3, 9, 7, 6, 9, 1, 3, 3, 8, 3, 3, 6, 9, 2, 4, 5, 8, 5, 1, 1, 4, 4, 3, 1, 0, 7, 7, 0, 7, 9, 9, 4, 8, 5, 5, 9, 0, 8, 2, 1, 6, 8, 4, 8, 0, 4, 0, 6, 1, 7, 3, 2, 6, 7, 2, 6, 9, 3, 1, 4, 6, 8, 5, 9, 8, 0, 6, 2, 1, 7, 3, 4, 1, 0, 5, 4, 3, 1, 1, 7, 4, 9, 9, 4, 8, 4, 0, 2, 4, 5, 1, 1, 6, 4, 7, 1, 9, 4, 2, 4, 1, 5, 5, 3, 8, 3, 1, 4, 5, 6, 8, 9, 4, 1, 5, 3, 8, 0, 1, 2, 5, 1, 2, 8, 3, 4, 4, 0, 8, 8, 3, 3, 1, 2, 3, 5, 9, 6, 3, 2, 6, 1, 3, 6, 0, 7, 2, 1, 7, 1, 4, 2, 4, 2, 1, 7, 9, 6, 1, 1, 2, 4, 3, 1, 7, 7, 4, 7, 0, 9, 3, 1, 3, 1, 0, 7, 7, 0, 3, 5, 5, 2, 9, 6, 6, 9, 2, 8, 3, 6, 2, 2, 5, 6, 0, 8, 2, 9, 2, 8, 8, 8, 8, 7, 4, 7, 3, 0, 6, 6, 3, 2, 1, 5, 2, 2, 9, 3, 0, 0, 5, 7, 8, 3, 4, 4, 6, 0, 2, 9, 1, 4, 7, 4, 7, 3, 9, 8, 8, 4, 7, 1, 2, 1, 2, 2, 3, 7, 3, 2, 3, 9, 1, 7, 4, 0, 3, 5, 5, 8, 6, 3, 0, 6, 7, 6, 6, 3, 2, 7, 9, 1, 1, 2, 4, 6, 4, 9, 5, 1, 3, 3, 4, 7, 8, 9, 1, 1, 0, 9, 1, 4, 4, 5, 4, 0, 6, 2, 3, 3, 1, 5, 1, 2, 0, 3, 8, 1, 2, 6, 7, 1, 6, 3, 3, 7, 0, 1, 2, 2, 0, 8, 9, 9, 0, 2, 5, 1, 9, 7, 8, 1, 0, 4, 1, 9, 9, 5, 4, 2, 6, 8, 1, 3, 7, 5, 4, 4, 1, 8, 1, 3, 8, 1, 2, 8, 8, 0, 6, 2, 1, 1, 9, 1, 5, 3, 4, 8, 9, 5, 0, 9, 2, 2, 4, 9, 2, 1, 9, 2, 4, 9, 4, 4, 0, 9, 9, 2, 2, 3, 3, 4, 3, 5, 7, 3, 5, 8, 1, 2, 4, 4, 6, 4, 9, 5, 1, 0, 6, 9, 5, 9, 5, 9, 7, 3, 8, 0, 3, 9, 1, 3, 6, 7, 8, 5, 9, 7, 9, 6, 3, 6, 3, 7, 4, 6, 5, 3, 5, 4, 7, 8, 9, 8, 0, 7, 6, 9, 8, 7, 3, 7, 1, 9, 5, 2, 7, 3, 5, 1, 1, 2, 1, 4, 7, 4, 7, 5, 4, 5, 4, 0, 8, 3, 6, 9, 6, 0, 2, 7, 4, 4, 4, 4, 6, 6, 4, 7, 9, 0, 4, 5, 5, 8, 7, 3, 9, 2, 7, 0, 2, 4, 1, 1, 1, 6, 9, 2, 8, 7, 2, 0, 1, 5, 0, 4, 1, 9, 0, 6, 0, 8, 6, 8, 1, 8, 0, 2, 3, 3, 2, 5, 6, 2, 1, 6, 1, 1, 3, 9, 9, 0, 8, 0, 5, 4, 0, 2, 8, 2, 2, 9, 8, 4, 0, 4, 5, 8, 5, 1, 2, 1, 3, 1, 7, 9, 4, 7, 2, 0, 5, 3, 8, 6, 2, 5, 4, 1, 9, 2, 1, 5, 8, 1, 0, 2, 4, 4, 3, 6, 8, 8, 2, 4, 0, 5, 0, 4, 4, 7, 9, 3, 4, 1, 5, 9, 2, 3, 5, 8, 8, 0, 5, 5, 3, 6, 6, 0, 1, 6, 0, 3, 5, 4, 4, 1, 2, 9, 1, 4, 6, 9, 9, 3, 9, 8, 4, 4, 3, 1, 3, 1, 3, 8, 7, 9, 4, 8, 8, 2, 9, 9, 1, 4, 5, 6, 0, 5, 2, 2, 6, 1, 5, 5, 2, 4, 9, 6, 2, 7, 9, 2, 2, 1, 1, 2, 8, 3, 7, 7, 4, 1, 7, 1, 7, 6, 7, 2, 2, 7, 3, 1, 7, 5, 8, 2, 6, 2, 2, 5, 6, 5, 0, 9, 2, 4, 3, 3, 9, 7, 6, 6, 8, 0, 4, 1, 3, 8, 2, 9, 1, 8, 0, 6, 7, 7, 1, 0, 5, 5, 2, 0, 2, 2, 0, 2, 4, 5, 8, 0, 9, 9, 4, 6, 5, 4, 9, 1, 8, 3, 4, 9, 9, 1, 2, 2, 8, 1, 4, 6, 4, 0, 9, 9, 8, 3, 8, 4, 0, 2, 5, 1, 9, 6, 2, 9, 9, 0, 9, 6, 0, 6, 2, 5, 4, 2, 3, 8, 4, 3, 5, 0, 3, 8, 5, 3, 5, 8, 6, 5, 7, 6, 3, 3, 9, 6, 1, 1, 2, 9, 0, 4, 3, 3, 6, 9, 5, 2, 3, 7, 7, 7, 8, 1, 9, 8, 3, 0, 7, 2, 7, 9, 4, 5, 4, 9, 3, 2, 1, 4, 0, 2, 3, 8, 5, 9, 6, 8, 0, 0, 1, 1, 4, 7, 3, 9, 0, 0, 0, 6, 6, 2, 3, 7, 8, 4, 7, 7, 9, 2, 4, 1, 6, 5, 3, 4, 8, 9, 1, 9, 4, 0, 9, 8, 4, 8, 7, 7, 0, 7, 8, 8, 6, 0, 4, 8, 8, 2, 4, 7, 6, 6, 6, 4, 9, 1, 8, 8, 2, 3, 6, 3, 0, 0, 3, 7, 6, 9, 7, 9, 9, 5, 4, 5, 3, 6, 1, 2, 3, 7, 3, 3, 6, 0, 3, 3, 8, 4, 3, 6, 3, 5, 0, 2, 0, 9, 0, 7, 4, 6, 9, 3, 5, 1, 9, 6, 1, 4, 5, 4, 5, 0, 5, 4, 5, 2, 1, 2, 9, 1, 9, 9, 4, 0, 8, 4, 5, 2, 9, 2, 1, 2, 1, 7, 3, 6, 8, 8, 4, 9, 1, 9, 8, 5, 7, 5, 1, 1, 8, 6, 5, 0, 4, 4, 7, 2, 3, 5, 6, 8, 8, 6, 2, 3, 1, 0, 5, 8, 9, 2, 9, 6, 7, 0, 4, 8, 7, 1, 7, 4, 1, 0, 3, 7, 2, 0, 0, 9, 1, 7, 0, 7, 8, 4, 7, 2, 0, 4, 6, 0, 3, 1, 1, 8, 3, 9, 6, 7, 4, 1, 2, 3, 0, 8, 7, 3, 9, 6, 9, 3, 5, 0, 2, 7, 4, 5, 1, 3, 5, 8, 0, 8, 8, 1, 5, 0, 3, 0, 3, 1, 4, 0, 3, 7, 2, 9, 1, 5, 0, 7, 0, 4, 3, 1, 9, 8, 7, 7, 1, 4, 9, 9, 3, 7, 1, 7, 9, 0, 2, 0, 3, 3, 7, 6, 9, 2, 3, 3, 7, 7, 0, 0, 7, 5, 2, 9, 5, 7, 4, 4, 2, 6, 6, 1, 9, 6, 8, 2, 9, 0, 8, 3, 1, 1, 6, 3, 5, 1, 1, 1, 3, 1, 2, 3, 0, 2, 0, 1, 3, 5, 5, 7, 4, 9, 9, 6, 9, 6, 8, 3, 6, 6, 8, 5, 1, 4, 2, 4, 9, 5, 1, 8, 9, 0, 2, 4, 9, 5, 7, 1, 8, 3, 5, 6, 9, 8, 7, 1, 1, 6, 7, 6, 3, 2, 2, 0, 8, 9, 2, 5, 1, 0, 8, 1, 4, 5, 7, 9, 6, 9, 0, 6, 1, 5, 5, 8, 3, 8, 2, 6, 5, 0, 7, 4, 6, 1, 3, 4, 7, 3, 2, 3, 4, 2, 5, 2, 7, 1, 7, 2, 6, 4, 1, 5, 2, 8, 6, 0, 1, 8, 2, 5, 7, 7, 6, 9, 3, 5, 2, 4, 2, 4, 0, 8, 8, 3, 9, 9, 2, 7, 5, 1, 6, 3, 6, 0, 8, 6, 7, 3, 6, 4, 9, 4, 6, 6, 3, 0, 4, 1, 0, 1, 4, 6, 2, 9, 1, 1, 0, 6, 3, 9, 5, 6, 5, 6, 5, 8, 4, 6, 4, 3, 9, 1, 3, 4, 1, 9, 1, 2, 1, 1, 9, 3, 5, 4, 0, 9, 3, 6, 1, 7, 5, 5, 3, 3, 0, 1, 3, 7, 5, 8, 6, 5, 1, 0, 8, 7, 3, 4, 5, 7, 9, 8, 1, 4, 9, 9, 2, 8, 6, 3, 7, 0, 0, 6, 7, 5, 8, 6, 0, 4, 3, 9, 1, 3, 0, 4, 3, 3, 5, 5, 6, 3, 0, 2, 3, 4, 2, 3, 0, 9, 9, 4, 7, 2, 8, 4, 7, 0, 6, 2, 8, 5, 2, 8, 5, 7, 8, 0, 8, 2, 9, 8, 8, 2, 5, 5, 7, 6, 4, 0, 8, 4, 8, 2, 7, 4, 5, 2, 0, 3, 2, 9, 6, 7, 2, 5, 1, 1, 1, 2, 3, 6, 7, 8, 7, 6, 4, 8, 9, 4, 8, 6, 3, 8, 3, 1, 0, 6, 2, 2, 5, 6, 9, 8, 8, 1, 4, 1, 7, 8, 4, 6, 1, 8, 4, 5, 1, 2, 8, 0, 8, 5, 9, 2, 4, 2, 0, 3, 7, 0, 1, 0, 2, 5, 7, 6, 7, 9, 4, 2, 6, 2, 4, 4, 8, 0, 4, 4, 5, 8, 0, 6, 8, 9, 8, 5, 6, 9, 0, 4, 8, 7, 1, 3, 4, 6, 8, 0, 9, 1, 3, 3, 6, 9, 8, 7, 1, 0, 6, 7, 1, 7, 5, 2, 7, 9, 1, 8, 5, 2, 4, 9, 4, 7, 2, 2, 3, 4, 9, 1, 9, 2, 1, 7, 9, 4, 4, 6, 6, 7, 2, 7, 8, 0, 1, 9, 7, 1, 1, 7, 5, 3, 3, 5, 1, 3, 7, 6, 1, 3, 8, 7, 8, 9, 6, 0, 0, 2, 8, 8, 8, 3, 7, 1, 3, 0, 8, 4, 4, 3, 8, 9, 2, 3, 9, 7, 1, 1, 7, 0, 4, 9, 6, 5, 9, 1, 3, 5, 2, 0, 2, 4, 6, 7, 0, 7, 1, 4, 6, 4, 5, 4, 9, 4, 1, 7, 9, 5, 3, 3, 8, 2, 3, 6, 2, 2, 1, 1, 1, 1, 1, 6, 9, 8, 4, 3, 7, 1, 6, 4, 9, 0, 4, 7, 4, 2, 4, 0, 7, 0, 1, 9, 9, 8, 6, 0, 0, 4, 1, 6, 8, 2, 2, 3, 3, 4, 3, 2, 2, 1, 7, 5, 4, 4, 0, 4, 3, 1, 7, 3, 1, 0, 1, 2, 5, 4, 2, 1, 0, 1, 8, 9, 1, 4, 8, 3, 7, 9, 3, 6, 8, 8, 3, 2, 1, 1, 0, 4, 2, 9, 2, 4, 3, 7, 9, 1, 5, 2, 4, 9, 0, 3, 8, 5, 3, 8, 0, 9, 4, 6, 2, 5, 0, 2, 7, 9, 6, 6, 8, 6, 6, 8, 6, 9, 1, 7, 2, 5, 4, 9, 0, 7, 2, 7, 6, 7, 0, 6, 5, 6, 4, 7, 2, 0, 9, 9, 2, 2, 9, 4, 4, 2, 3, 3, 2, 1, 7, 0, 9, 6, 4, 1, 3, 8, 7, 4, 6, 4, 2, 5, 1, 8, 7, 3, 7, 1, 5, 3, 0, 9, 1, 4, 0, 6, 3, 5, 6, 0, 4, 9, 7, 5, 1, 6, 8, 9, 5, 5, 7, 9, 3, 8, 3, 8, 1, 5, 3, 5, 0, 5, 5, 2, 8, 6, 7, 7, 7, 3, 7, 0, 5, 9, 0, 2, 5, 5, 3, 1, 7, 7, 8, 6, 5, 9, 3, 8, 9, 5, 3, 7, 9, 1, 7, 0, 0, 3, 7, 2, 5, 8, 1, 8, 6, 3, 9, 5, 7, 5, 3, 8, 6, 8, 8, 1, 4, 8, 4, 5, 8, 3, 0, 6, 2, 7, 3, 3, 2, 1, 0, 7, 3, 4, 0, 3, 9, 3, 2, 8, 9, 0, 3, 8, 0, 7, 6, 5, 4, 7, 3, 0, 0, 8, 6, 2, 5, 1, 1, 0, 0, 4, 4, 0, 1, 2, 3, 2, 7, 7, 8, 5, 2, 8, 7, 6, 9, 1, 4, 1, 6, 4, 2, 4, 3, 5, 4, 3, 9, 5, 0, 1, 5, 3, 8, 9, 1, 9, 7, 9, 5, 5, 2, 7, 4, 0, 0, 1, 1, 1, 0, 4, 4, 7, 6, 3, 8, 0, 4, 3, 0, 6, 1, 9, 6, 1, 3, 8, 1, 2, 5, 6, 2, 4, 3, 6, 0, 1, 9, 7, 6, 6, 8, 9, 2, 2, 5, 8, 3, 1, 0, 0, 7, 6, 6, 2, 1, 0, 9, 2, 1, 8, 6, 9, 0, 6, 0, 0, 0, 6, 3, 5, 9, 3, 9, 5, 5, 8, 5, 3, 0, 4, 0, 3, 9, 6, 8, 2, 3, 1, 2, 1, 1, 5, 6, 9, 8, 0, 6, 6, 5, 5, 3, 8, 6, 2, 1, 4, 5, 4, 3, 7, 8, 5, 0, 9, 3, 5, 1, 1, 0, 9, 4, 7, 0, 1, 7, 0, 1, 6, 1, 4, 5, 6, 6, 5, 7, 8, 4, 2, 7, 2, 5, 3, 7, 0, 7, 7, 9, 6, 4, 2, 8, 3, 7, 8, 3, 9, 5, 8, 9, 9, 8, 6, 2, 8, 4, 2, 3, 6, 1, 1, 8, 9, 3, 4, 0, 7, 9, 6, 9, 1, 4, 1, 3, 4, 9, 3, 1, 4, 7, 7, 4, 7, 2, 9, 3, 0, 8, 8, 8, 4, 0, 4, 4, 1, 5, 2, 8, 3, 4, 9, 5, 2, 8, 1, 5, 3, 7, 9, 4, 2, 5, 6, 2, 5, 9, 3, 5, 9, 2, 1, 9, 5, 3, 0, 6, 9, 8, 4, 0, 4, 5, 2, 9, 0, 1, 0, 3, 1, 6, 5, 8, 1, 5, 5, 5, 0, 3, 5, 5, 9, 2, 8, 7, 0, 4, 9, 1, 9, 7, 7, 5, 5, 2, 2, 9, 1, 8, 6, 2, 3, 9, 6, 2, 1, 9, 1, 3, 5, 5, 0, 1, 8, 3, 3, 7, 6, 6, 0, 1, 4, 0, 6, 9, 8, 1, 2, 8, 9, 5, 9, 7, 3, 7, 9, 0, 1, 3, 0, 4, 6, 1, 0, 2, 5, 6, 4, 4, 1, 1, 5, 4, 3, 6, 0, 4, 9, 2, 6, 2, 7, 1, 7, 9, 4, 0, 0, 3, 8, 2, 2, 3, 1, 6, 0, 5, 7, 7, 9, 2, 6, 7, 7, 7, 8, 6, 8, 8, 4, 6, 8, 4, 1, 2, 8, 3, 3, 9, 4, 0, 3, 7, 3, 2, 3, 3, 7, 3, 4, 0, 6, 2, 0, 8, 1, 5, 3, 5, 4, 1, 7, 1, 5, 7, 5, 7, 3, 2, 2, 7, 3, 7, 3, 7, 8, 5, 2, 5, 2, 9, 6, 5, 3, 6, 7, 4, 1, 7, 1, 5, 2, 3, 6, 3, 2, 4, 2, 6, 9, 4, 3, 8, 0, 6, 2, 1, 6, 9, 3, 9, 1, 9, 3, 2, 1, 8, 4, 4, 6, 7, 8, 6, 4, 7, 7, 8, 6, 9, 7, 1, 9, 4, 0, 5, 4, 6, 4, 1, 2, 3, 0, 0, 2, 6, 6, 5, 7, 0, 8, 6, 4, 4, 9, 0, 7, 2, 4, 2, 1, 8, 8, 5, 9, 2, 7, 1, 8, 8, 3, 2, 9, 6, 0, 1, 2, 7, 1, 0, 8, 3, 6, 0, 5, 3, 6, 2, 8, 7, 0, 1, 4, 2, 1, 1, 4, 4, 4, 4, 7, 1, 6, 2, 9, 9, 0, 0, 1, 8, 8, 4, 3, 4, 2, 0, 6, 1, 6, 1, 2, 2, 2, 1, 6, 9, 7, 8, 1, 0, 0, 2, 1, 0, 6, 0, 1, 6, 2, 5, 1, 7, 4, 8, 2, 1, 4, 3, 8, 3, 9, 9, 4, 9, 3, 4, 7, 2, 7, 5, 7, 0, 4, 3, 3, 2, 6, 7, 6, 0, 0, 6, 7, 7, 0, 5, 5, 8, 1, 0, 4, 0, 2, 8, 1, 5, 0, 8, 8, 0, 3, 2, 7, 7, 3, 6, 4, 9, 5, 5, 9, 2, 4, 2, 8, 4, 6, 8, 6, 5, 0, 0, 8, 7, 6, 1, 7, 1, 1, 2, 7, 4, 0, 0, 7, 7, 6, 3, 8, 6, 4, 2, 0, 9, 4, 0, 5, 7, 8, 2, 9, 4, 7, 1, 1, 3, 6, 6, 2, 9, 1, 9, 9, 8, 3, 6, 9, 5, 4, 6, 2, 4, 6, 7, 7, 0, 6, 6, 4, 9, 8, 3, 5, 3, 4, 9, 0, 0, 5, 2, 5, 0, 7, 1, 1, 1, 6, 7, 6, 7, 9, 6, 6, 4, 1, 4, 3, 1, 1, 2, 2, 4, 1, 0, 8, 7, 6, 3, 4, 0, 0, 6, 3, 3, 8, 7, 1, 7, 1, 1, 3, 6, 0, 9, 4, 7, 5, 4, 1, 4, 8, 9, 5, 3, 5, 1, 9, 8, 2, 7, 3, 9, 9, 0, 1, 0, 2, 9, 3, 9, 3, 3, 6, 2, 9, 9, 8, 3, 7, 4, 0, 4, 7, 8, 4, 9, 4, 1, 9, 7, 5, 9, 2, 8, 2, 2, 0, 2, 2, 3, 8, 4, 6, 8, 4, 8, 2, 4, 6, 7, 9, 3, 3, 9, 4, 3, 1, 4, 4, 7, 0, 5, 9, 6, 0, 4, 4, 4, 4, 6, 1, 2, 3, 2, 6, 4, 5, 9, 6, 8, 5, 6, 0, 8, 6, 4, 1, 8, 6, 3, 2, 5, 4, 5, 5, 4, 7, 7, 0, 7, 8, 2, 2, 3, 7, 0, 1, 8, 0, 7, 1, 9, 8, 7, 5, 5, 9, 1, 7, 5, 4, 3, 1, 2, 2, 1, 6, 6, 0, 1, 1, 4, 0, 7, 4, 2, 4, 0, 6, 4, 7, 6, 7, 5, 3, 4, 6, 5, 0, 1, 8, 8, 2, 3, 3, 5, 7, 8, 2, 8, 5, 7, 1, 1, 0, 1, 3, 7, 8, 5, 0, 7, 1, 1, 0, 1, 1, 4, 5, 2, 7, 6, 2, 3, 0, 2, 8, 5, 4, 6, 9, 9, 2, 1, 3, 6, 4, 1, 8, 2, 4, 0, 5, 1, 0, 3, 2, 6, 4, 4, 3, 9, 6, 1, 6, 5, 7, 9, 2, 0, 2, 6, 0, 1, 4, 3, 3, 2, 8, 8, 0, 8, 8, 9, 0, 9, 6, 7, 6, 3, 3, 3, 4, 7, 7, 7, 4, 9, 0, 6, 9, 8, 4, 2, 9, 2, 8, 1, 0, 0, 7, 8, 3, 3, 3, 1, 3, 7, 8, 1, 3, 1, 6, 7, 5, 2, 4, 7, 5, 9, 5, 3, 4, 9, 9, 1, 6, 7, 0, 1, 3, 2, 0, 9, 4, 8, 2, 2, 0, 2, 8, 1, 2, 1, 6, 8, 8, 9, 1, 2, 1, 3, 5, 1, 0, 9, 4, 4, 8, 3, 8, 5, 9, 7, 6, 0, 2, 0, 0, 0, 5, 8, 3, 1, 5, 3, 3, 8, 5, 1, 8, 2, 4, 4, 9, 9, 6, 2, 3, 3, 5, 6, 4, 8, 0, 9, 2, 8, 3, 6, 9, 5, 9, 2, 9, 4, 9, 1, 2, 8, 2, 0, 7, 0, 4, 1, 1, 5, 7, 0, 9, 9, 1, 9, 5, 9, 2, 5, 0, 4, 1, 0, 8, 9, 0, 9, 9, 8, 9, 4, 2, 5, 7, 9, 8, 9, 8, 0, 9, 9, 6, 8, 9, 9, 5, 9, 8, 6, 1, 0, 3, 3, 5, 2, 1, 6, 3, 0, 2, 8, 3, 5, 6, 2, 3, 0, 2, 2, 6, 4, 3, 5, 5, 1, 7, 2, 1, 6, 9, 1, 3, 9, 5, 5, 1, 6, 2, 2, 8, 6, 7, 1, 4, 6, 0, 6, 0, 3, 3, 2, 8, 3, 6, 8, 9, 2, 5, 3, 9, 5, 4, 5, 2, 0, 5, 6, 3, 2, 8, 3, 9, 9, 5, 7, 9, 4, 6, 7, 1, 3, 1, 3, 6, 6, 0, 9, 0, 1, 9, 4, 2, 8, 3, 0, 1, 6, 9, 7, 5, 3, 4, 7, 4, 9, 4, 4, 3, 6, 3, 1, 1, 7, 6, 9, 1, 8, 4, 1, 1, 9, 9, 8, 3, 6, 8, 1, 6, 0, 4, 1, 3, 9, 7, 4, 9, 5, 1, 0, 0, 1, 1, 6, 2, 1, 9, 8, 4, 0, 3, 6, 4, 9, 0, 7, 1, 6, 5, 7, 5, 2, 5, 1, 8, 5, 4, 7, 0, 5, 7, 2, 2, 5, 8, 1, 0, 4, 3, 7, 1, 0, 5, 1, 3, 0, 0, 6, 0, 7, 3, 1, 8, 3, 9, 7, 0, 0, 8, 4, 5, 9, 8, 3, 2, 7, 3, 9, 9, 2, 1, 1, 3, 7, 5, 3, 1, 9, 8, 2, 2, 2, 8, 8, 5, 7, 3, 8, 9, 8, 8, 6, 8, 2, 3, 9, 7, 5, 6, 2, 9, 2, 8, 8, 1, 6, 8, 8, 7, 9, 1, 8, 0, 1, 7, 2, 0, 7, 5, 1, 9, 0, 2, 0, 9, 8, 6, 2, 3, 0, 3, 8, 0, 2, 1, 1, 1, 1, 4, 2, 9, 7, 2, 5, 1, 1, 3, 1, 9, 9, 9, 1, 0, 2, 0, 2, 1, 1, 4, 6, 4, 1, 5, 4, 9, 9, 7, 9, 5, 6, 2, 6, 2, 8, 0, 6, 9, 5, 3, 9, 7, 3, 1, 4, 8, 5, 3, 4, 3, 4, 7, 7, 5, 0, 7, 4, 8, 8, 1, 5, 3, 9, 5, 9, 3, 6, 9, 0, 3, 6, 3, 9, 8, 2, 8, 1, 2, 8, 6, 8, 5, 5, 3, 9, 4, 9, 2, 5, 1, 5, 1, 9, 4, 1, 4, 6, 3, 5, 9, 1, 2, 2, 3, 3, 0, 2, 9, 0, 0, 9, 9, 6, 0, 4, 3, 7, 8, 4, 1, 4, 9, 7, 2, 7, 9, 9, 3, 9, 5, 1, 1, 8, 7, 5, 1, 9, 5, 5, 5, 9, 9, 5, 9, 5, 1, 9, 0, 9, 7, 5, 4, 9, 2, 0, 1, 0, 5, 1, 4, 9, 3, 3, 6, 1, 5, 2, 5, 3, 2, 0, 9, 2, 6, 6, 0, 1, 2, 0, 3, 0, 2, 5, 5, 7, 9, 5, 5, 0, 8, 9, 5, 0, 3, 2, 5, 4, 0, 8, 8, 4, 6, 8, 8, 4, 5, 4, 8, 5, 4, 9, 2, 2, 1, 2, 6, 8, 8, 9, 0, 3, 6, 6, 4, 3, 8, 8, 7, 2, 2, 0, 0, 4, 3, 9, 9, 1, 9, 8, 6, 6, 4, 2, 6, 9, 2, 8, 5, 4, 5, 3, 9, 4, 9, 2, 1, 8, 3, 4, 0, 3, 8, 7, 9, 3, 4, 6, 5, 6, 2, 1, 9, 2, 6, 0, 0, 0, 1, 8, 8, 7, 9, 8, 2, 0, 4, 7, 7, 5, 0, 5, 6, 4, 6, 7, 4, 3, 0, 7, 5, 0, 7, 4, 2, 0, 8, 9, 9, 4, 2, 4, 6, 7, 8, 7, 6, 9, 4, 1, 3, 7, 3, 0, 8, 7, 9, 6, 1, 3, 9, 2, 2, 9, 2, 1, 8, 3, 2, 9, 6, 8, 4, 0, 1, 2, 8, 4, 5, 2, 7, 8, 1, 1, 3, 0, 3, 5, 7, 0, 3, 1, 9, 3, 6, 3, 1, 7, 9, 3, 0, 8, 4, 8, 2, 4, 5, 2, 9, 7, 3, 9, 0, 9, 9, 6, 4, 2, 4, 7, 2, 1, 1, 6, 7, 4, 7, 5, 9, 6, 8, 2, 1, 9, 4, 5, 9, 6, 1, 3, 2, 5, 9, 9, 3, 6, 1, 1, 4, 6, 9, 7, 2, 1, 5, 1, 4, 6, 3, 4, 1, 1, 0, 3, 1, 6, 8, 4, 9, 0, 7, 3, 0, 4, 9, 0, 6, 6, 6, 3, 6, 7, 7, 2, 8, 6, 0, 8, 3, 0, 2, 9, 8, 5, 2, 5, 3, 9, 8, 0, 0, 1, 9, 5, 1, 3, 9, 6, 0, 1, 4, 1, 7, 1, 2, 3, 7, 9, 9, 4, 9, 9, 3, 9, 2, 8, 2, 7, 1, 3, 0, 9, 1, 0, 1, 7, 7, 9, 6, 9, 9, 9, 2, 1, 6, 1, 3, 5, 7, 1, 9, 7, 6, 4, 5, 7, 6, 6, 9, 9, 5, 3, 6, 2, 9, 8, 1, 2, 2, 5, 5, 2, 3, 7, 2, 1, 0, 1, 0, 4, 7, 2, 8, 2, 8, 3, 5, 1, 7, 9, 1, 1, 2, 4, 7, 8, 4, 0, 3, 0, 7, 8, 8, 4, 7, 7, 8, 5, 8, 6, 9, 8, 1, 3, 8, 0, 3, 1, 7, 9, 3, 5, 1, 6, 5, 7, 4, 9, 3, 5, 4, 7, 1, 2, 0, 8, 1, 6, 0, 7, 3, 4, 7, 8, 9, 6, 0, 8, 6, 4, 8, 7, 7, 9, 3, 8, 0, 9, 7, 2, 3, 4, 0, 2, 1, 0, 5, 5, 5, 7, 2, 4, 0, 7, 2, 8, 3, 0, 8, 7, 8, 4, 0, 8, 4, 4, 5, 8, 5, 6, 6, 3, 0, 9, 3, 1, 6, 8, 9, 8, 4, 9, 5, 8, 9, 1, 2, 8, 8, 6, 8, 1, 3, 7, 9, 0, 1, 1, 4, 7, 0, 9, 1, 7, 4, 5, 7, 1, 2, 1, 1, 3, 9, 6, 4, 1, 2, 4, 8, 7, 6, 6, 9, 3, 1, 0, 6, 2, 8, 0, 5, 4, 3, 8, 4, 6, 6, 2, 7, 9, 5, 1, 3, 2, 4, 3, 6, 1, 9, 4, 4, 7, 6, 8, 4, 1, 9, 9, 2, 7, 8, 0, 1, 3, 6, 1, 3, 4, 1, 1, 1, 5, 6, 0, 7, 0, 7, 2, 3, 2, 5, 2, 2, 9, 4, 9, 8, 1, 3, 1, 5, 1, 2, 7, 4, 0, 0, 0, 8, 2, 2, 9, 2, 2, 3, 9, 9, 2, 7, 5, 1, 3, 4, 9, 4, 1, 9, 5, 6, 2, 8, 3, 1, 2, 8, 4, 9, 9, 8, 7, 0, 7, 7, 2, 3, 8, 9, 0, 3, 9, 9, 8, 4, 1, 0, 6, 0, 9, 6, 8, 6, 1, 1, 9, 8, 9, 2, 3, 5, 5, 9, 4, 2, 1, 9, 4, 3, 9, 6, 0, 4, 0, 6, 0, 1, 2, 3, 4, 7, 8, 9, 0, 1, 2, 6, 4, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 8, 3, 4, 7, 8, 6, 3, 4, 0, 9, 7, 1, 9, 3, 8, 4, 7, 2, 0, 9, 1, 4, 5, 4, 6, 2, 0, 6, 2, 1, 1, 1, 1, 7, 2, 4, 7, 5, 2, 9, 4, 5, 8, 4, 2, 9, 7, 0, 0, 7, 5, 1, 1, 7, 6, 6, 6, 8, 2, 2, 7, 7, 4, 0, 2, 4, 2, 1, 8, 9, 6, 1, 0, 5, 9, 6, 9, 5, 0, 2, 0, 8, 3, 9, 6, 3, 0, 1, 2, 3, 4, 5, 6, 7, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 5, 1, 2, 3, 4, 5, 6, 7, 6, 5, 4, 8, 9, 4, 7, 7, 3, 9, 4, 4, 3, 1, 5, 8, 2, 7, 4, 2, 1, 4, 4, 5, 5, 8, 6, 4, 4, 4, 1, 8, 7, 5, 5, 1, 6, 7, 1, 3, 6, 3, 3, 2, 2, 6, 9, 9, 6, 5, 5, 3, 3, 8, 1, 6, 5, 6, 8, 1, 9, 7, 6, 8, 2, 7, 4, 7, 0, 9, 0, 0, 3, 2, 9, 3, 0, 2, 0, 1, 0, 1, 0, 4, 0, 1, 0, 4, 7, 9, 6, 2, 6, 2, 2, 9, 9, 0, 1, 2, 3, 4, 5, 6, 7, 9, 9, 0, 1, 2, 3, 4, 5, 6, 7, 9, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 8, 0, 5, 6, 6, 0, 8, 0, 2, 3, 7, 9, 4, 7, 1, 9, 1, 7, 1, 4, 0, 0, 4, 1, 7, 5, 7, 1, 3, 3, 3, 6, 6, 9, 7, 4, 3, 0, 2, 5, 2, 6, 0, 8, 9, 4, 3, 5, 4, 8, 1, 5, 9, 0, 6, 4, 3, 6, 3, 8, 8, 1, 4, 7, 5, 7, 2, 2, 0, 0, 1, 7, 7, 9, 5, 9, 8, 9, 6, 9, 8, 2, 3, 6, 1, 2, 9, 8, 9, 5, 2, 6, 2, 4, 8, 4, 6, 5, 0, 1, 5, 6, 7, 8, 9, 0, 1, 2, 3, 6, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 7, 4, 2, 0, 9, 0, 1, 5, 8, 8, 0, 2, 7, 8, 4, 4, 6, 1, 0, 4, 5, 3, 9, 4, 2, 0, 5, 0, 1, 3, 2, 9, 8, 6, 0, 1, 1, 8, 0, 4, 7, 7, 6, 3, 6, 0, 7, 3, 5, 4, 2, 4, 1, 8, 3, 5, 6, 7, 0, 6, 7, 1, 2, 5, 8, 1, 9, 3, 8, 2, 8, 7, 6, 7, 1, 4, 6, 2, 9, 3, 0, 1, 2, 3, 4, 5, 6, 7, 0, 1, 2, 3, 4, 5, 0, 1, 2, 8, 9, 1, 4, 0, 9, 5, 0, 8, 0, 7, 7, 1, 1, 2, 9, 3, 6, 7, 2, 3, 8, 1, 2, 9, 8, 8, 7, 1, 7, 1, 1, 0, 3, 4, 2, 6, 9, 7, 4, 2, 7, 4, 9, 1, 0, 6, 8, 5, 5, 5, 3, 5, 9, 7, 4, 8, 5, 9, 6, 9, 3, 0, 3, 3, 9, 1, 8, 1, 6, 0, 6, 1, 2, 3, 4, 5, 6, 9, 6, 9, 0, 1, 2, 3, 4, 5, 6, 7, 6, 9, 0, 1, 2, 3, 4, 5, 6, 9, 8, 9, 0, 5, 3, 2, 9, 3, 2, 1, 4, 5, 5, 3, 3, 2, 1, 3, 9, 7, 2, 5, 2, 8, 9, 1, 8, 8, 9, 8, 1, 0, 0, 9, 7, 8, 9, 5, 0, 6, 1, 5, 7, 4, 6, 1, 2, 5, 0, 7, 9, 9, 0, 3, 8, 4, 6, 8, 1, 8, 6, 5, 9, 0, 0, 0, 3, 7, 1, 6, 4, 2, 6, 6, 0, 4, 5, 4, 1, 3, 8, 6, 3, 9, 9, 5, 9, 3, 9, 8, 5, 6, 4, 7, 6, 2, 2, 0, 9, 4, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 9, 1, 6, 0, 1, 2, 3, 4, 5, 6, 8, 7, 1, 3, 2, 6, 0, 7, 5, 9, 9, 6, 0, 9, 4, 1, 3, 2, 1, 2, 3, 8, 3, 2, 6, 8, 6, 8, 2, 7, 4, 8, 1, 8, 0, 5, 3, 9, 4, 1, 9, 2, 1, 9, 6, 7, 9, 0, 4, 6, 1, 7, 3, 8, 7, 2, 9, 0, 5, 8, 3, 9, 0, 5, 7, 1, 6, 1, 0, 9, 3, 3, 4, 4, 0, 6, 2, 5, 4, 2, 3, 4, 6, 0, 0, 2, 0, 1, 4, 5, 6, 9, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 8, 7, 1, 3, 7, 5, 2, 8, 0, 9, 5, 9, 9, 0, 9, 1, 1, 5, 8, 8, 6, 3, 2, 1, 8, 3, 2, 6, 5, 6, 5, 2, 1, 0, 5, 3, 1, 9, 2, 1, 9, 6, 0, 4, 6, 1, 7, 3, 8, 9, 2, 9, 6, 5, 8, 3, 3, 7, 1, 6, 1, 0, 9, 6, 2, 2, 9, 2, 3, 9, 4, 6, 0, 0, 2, 0, 1, 2, 3, 9, 3, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 8, 4, 5, 6, 7, 8, 9, 8, 6, 5, 0, 6, 8, 9, 4, 1, 9, 3, 8, 0, 4, 8, 9, 1, 4, 0, 5, 5, 6, 1, 5, 4, 0, 7, 6, 0, 1, 7, 0, 6, 8, 9, 5, 1, 7, 9, 8, 6, 0, 8, 1, 7, 7, 1, 3, 2, 3, 1, 4, 2, 0, 0, 7, 8, 4, 6, 4, 9, 5, 8, 4, 7, 2, 8, 6, 3, 6, 9, 6, 8, 0, 2, 4, 6, 9, 0, 2, 8, 5, 1, 5, 8, 9, 7, 8, 7, 2, 2, 5, 7, 9, 8, 2, 1, 5, 1, 3, 0, 1, 2, 8, 4, 5, 6, 7, 8, 3, 0, 8, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 0, 1, 2, 6, 5, 3, 0, 7, 0, 4, 8, 4, 3, 6, 7, 2, 3, 1, 2, 1, 2, 5, 6, 0, 8, 3, 0, 2, 7, 5, 7, 6, 2, 9, 1, 9, 0, 6, 0, 6, 0, 2, 0, 6, 1, 5, 8, 4, 3, 0, 1, 5, 4, 4, 8, 5, 7, 5, 7, 8, 3, 4, 8, 8, 5, 2, 5, 7, 1, 3, 8, 1, 0, 7, 8, 3, 6, 3, 4, 7, 7, 8, 2, 3, 4, 4, 3, 8, 6, 2, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 8, 3, 9, 5, 5, 2, 6, 8, 4, 9, 1, 7, 1, 2, 3, 5, 9, 6, 9, 1, 1, 1, 2, 9, 5, 6, 8, 1, 2, 0, 7, 7, 5, 8, 2, 9, 8, 9, 0, 4, 6, 7, 1, 3, 4, 5, 6, 0, 3, 6, 8, 7, 0, 4, 2, 7, 4, 7, 5, 4, 3, 4, 2, 8, 1, 5, 1, 2, 0, 2, 5, 6, 4, 3, 0, 0, 0, 3, 3, 5, 7, 0, 6, 4, 8, 8, 6, 3, 4, 6, 9, 9, 8, 2, 7, 7, 1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 0, 1, 2, 3, 4, 5, 6, 7, 8, 2, 1, 7, 2, 5, 0, 8, 0, 2, 7, 6, 8, 3, 6, 0, 2, 7, 6, 6, 1, 2, 2, 8, 7, 7, 4, 7, 7, 3, 7, 4, 5, 4, 2, 3, 8, 4, 1, 1, 9, 7, 4, 3, 7, 3, 3, 0, 2, 5, 5, 6, 6, 3, 8, 6, 5, 9, 9, 8, 4, 1, 0, 6, 6, 9, 6, 8, 8, 5, 6, 1, 1, 9, 8, 9, 2, 3, 5, 5, 9, 4, 2, 1, 9, 2, 9, 2, 0, 6, 0, 4, 0, 0, 1, 2, 3, 4, 7, 8, 9, 0, 1, 2, 3, 7, 8, 9, 0, 1, 2, 3, 4, 7, 8, 9, 7, 3, 0, 3, 1, 8, 7, 6, 4, 0, 2, 6, 8, 3, 2, 8, 1, 2, 0, 7, 1, 0, 4, 4, 5, 8, 0, 6, 2, 3, 1, 5, 1, 8, 5, 9, 4, 0, 7, 5, 8, 8, 3, 8, 9, 2, 6, 2, 5, 3, 1, 7, 3, 0, 1, 9, 9, 6, 0, 3, 9, 2, 8, 1, 4, 5, 5, 2, 9, 2, 5, 8, 9, 5, 0, 1, 2, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 1, 0, 4, 5, 6, 6, 3, 4, 4, 2, 9, 1, 0, 6, 4, 9, 7, 2, 3, 3, 9, 2, 0, 4, 3, 3, 9, 1, 5, 6, 3, 7, 7, 8, 4, 0, 2, 4, 0, 2, 4, 7, 8, 0, 7, 0, 6, 9, 3, 2, 8, 6, 7, 3, 7, 5, 1, 0, 8, 1, 6, 7, 2, 4, 7, 9, 5, 8, 6, 2, 6, 2, 8, 1, 7, 5, 0, 1, 1, 3, 2, 4, 9, 1, 8, 6, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 4, 0, 1, 2, 3, 4, 7, 8, 9, 5, 1, 7, 8, 9, 9, 8, 9, 8, 4, 1, 7, 7, 3, 3, 7, 6, 6, 6, 1, 9, 0, 1, 7, 6, 3, 2, 1, 7, 1, 3, 9, 1, 7, 6, 8, 4, 1, 4, 3, 6, 7, 6, 1, 4, 4, 7, 2, 4, 4, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 9, 0, 1, 2, 3, 4, 7, 8, 1, 3, 5, 1, 7, 7, 2, 1, 4, 8, 3, 4, 4, 3, 9, 7, 4, 1, 2, 3, 5, 9, 1, 6, 0, 1, 0, 0, 2, 7, 7, 1, 1, 4, 0, 4, 7, 3, 6, 5, 0, 3, 7, 8, 0, 6, 9, 2, 6, 5, 6, 6, 9, 0, 4, 0, 6, 6, 8, 4, 0, 9, 5, 1, 3, 7, 6, 9, 3, 0, 2, 2, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 4, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 2, 1, 7, 2, 5, 0, 8, 0, 2, 7, 8, 8, 3, 0, 6, 0, 2, 7, 6, 4, 1, 2, 8, 8, 7, 7, 4, 7, 7, 3, 7, 4, 5, 4, 3, 3, 8, 4, 5, 4, 1, 1, 9, 7, 4, 3, 7, 3, 3, 0, 2, 5, 5, 6, 3, 1, 5, 2, 5, 9, 9, 8, 4, 1, 0, 6, 0, 9, 6, 8, 8, 5, 6, 1, 1, 9, 8, 9, 2, 3, 5, 9, 9, 4, 2, 1, 9, 4, 9, 1, 3, 9, 2, 0, 6, 0, 4, 0, 6, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 3, 8, 0, 7, 1, 0, 7, 5, 5, 6, 9, 0, 1, 0, 0, 8, 3, 4, 3, 1, 5, 0, 0, 9, 6, 3, 4, 9, 3, 7, 6, 9, 2, 4, 5, 7, 2, 6, 4, 9, 4, 9, 4, 1, 2, 2, 5, 8, 1, 3, 2, 9, 4, 3, 8, 2, 2, 1, 2, 8, 6, 5, 1, 6, 7, 2, 1, 3, 9, 3, 8, 7, 5, 7, 0, 7, 4, 8, 8, 5, 0, 6, 6, 3, 7, 6, 9, 9, 4, 8, 4, 1, 6, 6, 6, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 7, 4, 0, 4, 0, 1, 7, 9, 5, 1, 4, 2, 9, 9, 4, 3, 7, 8, 2, 4, 4, 3, 3, 6, 9, 9, 5, 8, 6, 7, 0, 6, 8, 2, 6, 3, 9, 3, 2, 9, 6, 1, 7, 4, 8, 8, 9, 0, 9, 3, 9, 0, 5, 2, 9, 4, 1, 0, 3, 7, 5, 8, 7, 7, 8, 2, 9, 7, 1, 2, 6, 4, 2, 5, 2, 3, 6, 6, 5, 0, 0, 2, 8, 1, 6, 1, 0, 4, 3, 1, 6, 1, 9, 0, 1, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 0, 1, 2, 3, 4, 5, 6, 7, 9, 9, 8, 4, 0, 0, 7, 2, 4, 3, 8, 6, 6, 3, 2, 6, 3, 3, 3, 1, 4, 7, 8, 0, 3, 1, 9, 0, 1, 9, 1, 2, 7, 0, 1, 3, 8, 2, 9, 2, 7, 6, 5, 5, 9, 9, 8, 2, 9, 1, 3, 2, 3, 4, 3, 1, 9, 0, 9, 3, 6, 8, 7, 0, 1, 0, 5, 8, 2, 7, 7, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 7, 4, 8, 1, 5, 6, 5, 7, 2, 2, 6, 3, 3, 8, 6, 5, 4, 0, 9, 1, 7, 2, 9, 1, 5, 1, 3, 2, 2, 3, 0, 6, 4, 9, 7, 6, 9, 0, 9, 8, 1, 4, 0, 6, 1, 2, 6, 9, 2, 2, 9, 5, 5, 1, 0, 7, 7, 9, 6, 2, 9, 4, 7, 0, 2, 3, 4, 0, 0, 8, 8, 8, 5, 1, 3, 7, 4, 9, 8, 8, 9, 0, 9, 8, 9, 0, 2, 6, 5, 6, 7, 4, 7, 5, 4, 1, 3, 5, 3, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 6, 0, 1, 2, 4, 5, 6, 7, 8, 1, 7, 2, 4, 1, 4, 1, 4, 4, 6, 8, 4, 5, 3, 1, 8, 8, 3, 3, 5, 6, 7, 0, 6, 1, 6, 8, 7, 0, 1, 5, 0, 8, 6, 0, 1, 5, 8, 4, 2, 3, 9, 7, 6, 9, 1, 9, 0, 6, 7, 1, 2, 3, 9, 2, 4, 5, 5, 3, 7, 5, 3, 1, 8, 2, 2, 3, 0, 2, 9, 4, 9, 7, 0, 7, 7, 4, 9, 9, 2, 3, 9, 8, 3, 8, 6, 7, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 0, 7, 2, 6, 5, 5, 3, 7, 9, 6, 6, 6, 6, 4, 3, 8, 8, 3, 0, 1, 9, 2, 5, 4, 1, 9, 1, 2, 7, 0, 1, 3, 9, 2, 9, 2, 7, 4, 2, 6, 5, 5, 9, 9, 1, 1, 5, 7, 6, 8, 2, 9, 4, 3, 1, 9, 0, 9, 3, 6, 8, 7, 0, 1, 0, 5, 8, 2, 7, 7, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 3, 3, 4, 5, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 2, 1, 2, 1, 3, 9, 9, 8, 5, 3, 7, 0, 7, 7, 5, 7, 9, 9, 4, 7, 0, 3, 4, 1, 5, 8, 1, 4, 8, 4, 1, 8, 6, 6, 4, 6, 0, 5, 5, 3, 3, 5, 7, 2, 5, 9, 6, 9, 2, 6, 2, 1, 2, 0, 8, 3, 8, 3, 0, 8, 7, 4, 9, 5, 0, 9, 7, 0, 0, 4, 6, 0, 9, 1, 6, 2, 7, 6, 8, 3, 5, 2, 1, 8, 3, 8, 6, 1, 0, 2, 1, 4, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 7, 6, 4, 7, 6, 2, 3, 4, 8, 7, 8, 6, 9, 8, 3, 2, 2, 8, 4, 8, 5, 6, 5, 0, 2, 0, 1, 1, 2, 9, 6, 8, 2, 1, 0, 6, 8, 2, 9, 7, 6, 3, 9, 2, 7, 1, 8, 3, 8, 1, 9, 5, 5, 0, 1, 8, 9, 8, 2, 6, 0, 4, 5, 0, 2, 2, 8, 6, 7, 5, 9, 9, 3, 0, 3, 1, 4, 4, 0, 4, 9, 0, 8, 2, 3, 5, 6, 7, 8, 0, 1, 2, 3, 5, 6, 7, 8, 9, 0, 1, 2, 2, 5, 6, 7, 8, 9, 9, 7, 0, 9, 0, 1, 5, 8, 8, 0, 9, 3, 2, 7, 8, 4, 6, 1, 0, 4, 9, 4, 4, 0, 5, 0, 1, 6, 9, 3, 2, 9, 1, 6, 0, 8, 1, 8, 7, 7, 6, 2, 6, 0, 7, 2, 4, 1, 7, 0, 6, 7, 1, 4, 5, 8, 1, 8, 2, 8, 7, 6, 8, 9, 8, 6, 2, 9, 3, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 8, 9, 5, 7, 0, 3, 1, 6, 9, 4, 1, 5, 6, 4, 2, 7, 8, 1, 3, 4, 3, 4, 7, 2, 0, 5, 0, 8, 9, 2, 3, 2, 3, 5, 5, 7, 8, 4, 9, 9, 9, 8, 1, 9, 0, 7, 8, 3, 4, 8, 6, 3, 8, 0, 9, 6, 2, 8, 0, 1, 0, 6, 2, 3, 8, 9, 0, 7, 2, 3, 4, 5, 5, 2, 8, 5, 4, 6, 6, 6, 7, 9, 1, 9, 4, 1, 5, 3, 4, 7, 9, 4, 0, 0, 0, 1, 2, 3, 9, 8, 6, 7, 8, 9, 0, 8, 2, 3, 9, 3, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 9, 0, 1, 3, 1, 5, 1, 2, 4, 9, 8, 8, 6, 8, 0, 1, 1, 9, 2, 6, 6, 8, 7, 4, 2, 9, 7, 0, 2, 1, 0, 3, 6, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 4, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 8, 6, 5, 9, 7, 0, 2, 3, 4, 3, 8, 5, 1, 5, 2, 3, 0, 1, 2, 1, 3, 2, 6, 5, 3, 0, 7, 2, 7, 4, 6, 4, 0, 5, 9, 9, 5, 9, 5, 3, 1, 7, 4, 7, 6, 5, 4, 0, 0, 6, 6, 2, 0, 6, 3, 7, 7, 4, 4, 3, 9, 2, 8, 9, 6, 0, 9, 5, 3, 8, 0, 7, 1, 4, 0, 4, 8, 5, 2, 3, 9, 0, 1, 9, 1, 5, 1, 7, 4, 8, 6, 2, 1, 6, 8, 8, 0, 1, 2, 3, 4, 7, 8, 9, 0, 1, 9, 3, 4, 6, 7, 8, 9, 0, 1, 2, 3, 4, 7, 8, 9, 1, 4, 5, 3, 3, 0, 9, 5, 4, 5, 0, 2, 4, 6, 7, 0, 7, 7, 1, 6, 9, 1, 3, 6, 2, 3, 8, 2, 3, 8, 9, 5, 8, 8, 7, 1, 7, 1, 1, 0, 3, 4, 2, 6, 4, 7, 4, 2, 2, 4, 2, 9, 2, 7, 9, 2, 1, 6, 6, 5, 3, 4, 8, 5, 7, 6, 9, 0, 6, 3, 0, 4, 1, 6, 0, 0, 1, 2, 3, 4, 5, 6, 7, 0, 1, 2, 3, 4, 7, 8, 9, 0, 1, 2, 3, 4, 7, 2, 5, 1, 6, 4, 3, 9, 9, 0, 9, 9, 1, 6, 4, 3, 6, 2, 0, 9, 8, 6, 5, 7, 0, 0, 1, 7, 4, 3, 2, 4, 1, 3, 7, 6, 4, 7, 7, 7, 9, 8, 4, 3, 4, 2, 6, 3, 5, 8, 0, 5, 8, 7, 1, 3, 1, 7, 9, 6, 2, 0, 9, 1, 7, 3, 3, 9, 1, 6, 4, 3, 9, 8, 2, 1, 8, 6, 4, 1, 5, 5, 6, 5, 0, 1, 2, 3, 4, 5, 6, 7, 0, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 6, 9, 7, 0, 2, 3, 4, 3, 2, 5, 1, 3, 0, 1, 2, 8, 3, 2, 0, 7, 2, 6, 4, 0, 8, 9, 9, 8, 9, 5, 3, 8, 7, 4, 7, 0, 0, 6, 6, 6, 3, 7, 9, 2, 6, 9, 8, 7, 1, 4, 0, 4, 8, 5, 2, 3, 9, 0, 1, 9, 1, 5, 1, 7, 6, 1, 2, 1, 6, 8, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 0, 1, 2, 3, 5, 6, 7, 8, 1, 0, 4, 5, 6, 6, 3, 4, 4, 2, 8, 1, 0, 6, 4, 9, 7, 2, 9, 2, 0, 9, 3, 3, 9, 1, 5, 2, 3, 1, 6, 7, 3, 7, 8, 4, 0, 2, 4, 0, 2, 4, 7, 8, 0, 7, 0, 6, 9, 3, 2, 4, 8, 6, 0, 5, 7, 5, 1, 0, 8, 1, 6, 7, 2, 9, 7, 9, 5, 6, 5, 2, 6, 2, 8, 1, 7, 5, 5, 7, 3, 5, 0, 1, 1, 3, 8, 4, 9, 4, 5, 1, 8, 6, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 3, 5, 3, 2, 9, 3, 2, 1, 4, 5, 5, 2, 3, 2, 1, 3, 9, 7, 2, 1, 2, 8, 9, 1, 8, 8, 7, 8, 1, 0, 0, 6, 7, 7, 8, 7, 5, 0, 6, 1, 5, 7, 4, 6, 1, 2, 5, 0, 7, 9, 9, 0, 3, 4, 4, 9, 4, 1, 8, 6, 5, 9, 0, 0, 0, 3, 7, 1, 6, 4, 6, 0, 4, 5, 4, 1, 3, 8, 6, 3, 9, 9, 5, 9, 3, 7, 8, 5, 6, 4, 7, 6, 2, 2, 0, 9, 4, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 1, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 6, 4, 2, 6, 4, 7, 5, 5, 4, 7, 2, 9, 3, 9, 3, 8, 2, 0, 9, 5, 6, 0, 1, 0, 6, 5, 3, 5, 3, 8, 0, 0, 3, 4, 1, 5, 3, 0, 8, 3, 0, 6, 2, 7, 8, 1, 7, 1, 3, 8, 5, 4, 2, 0, 9, 7, 6, 7, 4, 1, 6, 2, 6, 7, 1, 9, 8, 0, 6, 9, 4, 9, 9, 6, 2, 3, 7, 1, 9, 2, 2, 5, 3, 7, 1, 0, 1, 2, 3, 4, 7, 8, 9, 0, 1, 2, 3, 4, 7, 8, 9, 0, 1, 7, 8, 9, 8, 9, 2, 6, 1, 3, 5, 4, 8, 2, 6, 4, 3, 4, 5, 9, 2, 0, 3, 9, 4, 9, 7, 3, 8, 7, 4, 4, 9, 8, 5, 8, 2, 6, 6, 2, 3, 1, 3, 2, 7, 3, 1, 9, 0, 1, 1, 3, 5, 0, 7, 8, 1, 5, 1, 4, 6, 0, 0, 4, 9, 1, 6, 6, 9, 0, 7, 6, 1, 1, 0, 1, 2, 3, 4, 2, 8, 3, 4, 6, 6, 2, 0, 1, 2, 2, 8, 6, 3, 9, 2, 1, 4, 3, 9, 6, 1, 7, 2, 4, 4, 5, 2, 0, 0, 1, 6, 6, 8, 2, 7, 7, 2, 4, 2, 1, 6, 1, 0, 6, 9, 8, 3, 9, 6, 3, 0, 8, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 6, 8, 9, 9, 0, 1, 2, 4, 4, 3, 7, 4, 8, 4, 0, 3, 8, 7, 5, 8, 2, 1, 7, 5, 3, 9, 5, 2, 5, 1, 1, 6, 2, 1, 3, 8, 6, 4, 2, 6, 2, 5, 5, 0, 2, 8, 0, 6, 8, 1, 7, 9, 1, 9, 2, 6, 7, 6, 6, 8, 7, 4, 9, 2, 1, 3, 3, 0, 5, 5, 8, 0, 3, 7, 9, 7, 0, 2, 7, 9, 1, 7, 8, 0, 3, 5, 3, 6, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 7, 8, 9, 6, 4, 2, 6, 4, 7, 8, 9, 2, 9, 3, 9, 3, 0, 0, 1, 0, 4, 2, 6, 3, 5, 3, 0, 3, 4, 1, 5, 3, 0, 8, 3, 0, 6, 1, 7, 8, 0, 9, 2, 6, 7, 1, 9, 6, 9, 4, 9, 9, 6, 7, 1, 2, 5, 3, 7, 8, 0, 1, 2, 4, 5, 6, 7, 8, 9, 0, 1, 3, 4, 5, 6, 7, 8, 0, 1, 3, 4, 7, 8, 9, 7, 5, 5, 1, 9, 9, 7, 1, 0, 0, 5, 9, 7, 1, 7, 3, 2, 3, 6, 8, 3, 2, 0, 0, 6, 1, 7, 5, 8, 6, 2, 9, 4, 8, 8, 7, 1, 0, 8, 7, 7, 5, 8, 5, 3, 4, 6, 1, 1, 5, 5, 0, 7, 2, 3, 6, 4, 1, 2, 4, 1, 5, 4, 2, 0, 4, 8, 6, 1, 9, 0, 2, 5, 6, 9, 3, 6, 3, 6, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 5, 6, 7, 8, 1, 0, 9, 5, 7, 5, 1, 8, 6, 4, 0, 4, 1, 9, 3, 8, 4, 4, 7, 0, 1, 9, 2, 8, 7, 8, 2, 5, 9, 6, 0, 6, 3, 5, 3, 3, 3, 9, 8, 1, 1, 0, 6, 1, 0, 0, 6, 2, 1, 1, 3, 2, 7, 7, 8, 8, 7, 8, 4, 6, 0, 2, 0, 7, 0, 3, 6, 8, 7, 1, 3, 9, 9, 3, 7, 2, 4, 9, 4, 3, 6, 2, 2, 5, 3, 2, 5, 5, 9, 4, 1, 7, 2, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 9, 9, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 0, 1, 2, 7, 5, 3, 4, 4, 0, 0, 6, 9, 6, 6, 5, 7, 2, 3, 9, 4, 9, 1, 4, 0, 7, 9, 5, 7, 2, 3, 1, 4, 4, 0, 9, 9, 6, 1, 8, 3, 3, 9, 3, 9, 8, 8, 4, 7, 7, 6, 2, 1, 9, 8, 7, 8, 8, 7, 2, 2, 3, 9, 3, 3, 5, 5, 0, 7, 4, 5, 6, 5, 1, 4, 1, 8, 2, 8, 2, 6, 1, 5, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 2, 8, 9, 0, 1, 2, 8, 4, 5, 6, 7, 8, 8, 0, 6, 0, 3, 2, 3, 7, 9, 4, 7, 1, 9, 7, 7, 1, 4, 0, 0, 1, 7, 5, 7, 1, 3, 3, 3, 1, 6, 9, 7, 1, 3, 0, 2, 6, 0, 8, 9, 7, 3, 5, 4, 8, 1, 5, 9, 0, 6, 3, 3, 8, 1, 4, 7, 5, 2, 0, 0, 1, 7, 8, 9, 6, 8, 8, 2, 3, 5, 1, 2, 9, 5, 2, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 0, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 6, 6, 7, 8, 9, 7, 4, 6, 1, 6, 0, 9, 9, 3, 7, 1, 2, 7, 5, 8, 6, 3, 0, 0, 0, 5, 5, 6, 0, 3, 8, 1, 0, 3, 0, 9, 7, 4, 9, 0, 9, 0, 7, 1, 7, 1, 6, 6, 5, 6, 0, 5, 7, 6, 4, 9, 9, 5, 3, 7, 4, 3, 0, 9, 6, 6, 1, 1, 3, 2, 1, 0, 0, 1, 2, 3, 4, 7, 8, 4, 0, 1, 8, 3, 4, 5, 6, 7, 8, 0, 1, 2, 3, 4, 7, 8, 9, 0, 8, 3, 9, 5, 5, 0, 6, 5, 4, 1, 7, 1, 3, 3, 5, 6, 9, 1, 1, 1, 2, 1, 2, 0, 7, 7, 5, 8, 2, 9, 8, 8, 7, 3, 4, 6, 8, 7, 0, 4, 8, 7, 7, 5, 4, 3, 4, 2, 8, 1, 5, 1, 0, 2, 3, 3, 1, 7, 0, 6, 8, 8, 3, 4, 9, 6, 8, 7, 7, 1, 0, 1, 7, 8, 9, 0, 1, 0, 9, 4, 5, 6, 7, 5, 0, 1, 2, 3, 4, 7, 8, 4, 7, 8, 6, 4, 1, 9, 3, 8, 4, 4, 7, 0, 1, 9, 2, 8, 7, 8, 2, 6, 0, 6, 6, 3, 5, 9, 9, 1, 4, 0, 6, 1, 0, 0, 6, 2, 1, 1, 7, 7, 8, 4, 6, 0, 7, 0, 3, 6, 8, 7, 1, 3, 2, 4, 9, 4, 2, 6, 4, 1, 7, 3, 6, 6, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FVE-HAjCG-sw",
        "outputId": "56fd087b-0e97-4693-8b7d-eac37b4d8f06"
      },
      "source": [
        "\n",
        "# Print the required Metrics\n",
        "# accuracy: (tp + tn) / (p + n)\n",
        "accuracy = accuracy_score(y_test, list2)\n",
        "print('Accuracy: %f' % accuracy)\n",
        "# precision tp / (tp + fp)\n",
        "precision = precision_score(y_test, list2,average='weighted')\n",
        "print('Precision: %f' % precision)\n",
        "# recall: tp / (tp + fn)\n",
        "recall = recall_score(y_test, list2,average='weighted')\n",
        "print('Recall: %f' % recall)\n",
        "# f1: 2 tp / (2 tp + fp + fn)\n",
        "f1 = f1_score(y_test, list2,average='weighted')\n",
        "print('F1 score: %f' % f1)\n",
        "# confusion matrix\n",
        "matrix = confusion_matrix(y_test, list2)\n",
        "print(matrix)"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.933800\n",
            "Precision: 0.934274\n",
            "Recall: 0.933800\n",
            "F1 score: 0.933843\n",
            "[[ 959    0    5    3    1    3    5    1    3    0]\n",
            " [   0 1103    4    4    0    1    4    1   17    1]\n",
            " [  11    2  955   20    6    0    9   10   18    1]\n",
            " [   2    4   17  936    0   19    1    6   14   11]\n",
            " [   0    0    5    0  922    0   10    1    7   37]\n",
            " [   6    2    3   24    4  814   15    3   16    5]\n",
            " [  12    3    1    2    6    9  918    2    5    0]\n",
            " [   1    6   18   11    3    1    0  936    1   51]\n",
            " [   6    4   10   13   14   10   12    5  869   31]\n",
            " [   4    5    3   11   40    5    1    9    5  926]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}